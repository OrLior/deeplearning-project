{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitted by:\n",
    " \n",
    "* Name 1: Adam Ginton\n",
    "* Name 2: Or Lior\n",
    "* Name 3: Sharon Edri\n",
    "\n",
    "## To be submitted by: Unknown yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# paint_the_circle Project\n",
    "\n",
    "This project identifies and paints circles given in RGB images. Specifically, each input image contains a rectangle, a triangle and a circle. The image is fed into a neural netork, whose outputs are the center and radius of the circle within the image. The domain considered is \"[0,1]x[0,1] \"box\". PyTorch is the platform used. The goal is to paint the circle.\n",
    "\n",
    "### **You are given the following files:**\n",
    "  Three directories named \"train\", \"validation\", and \"test\" each containing the following:\n",
    "  1. a directory called \"images\" with RGB png files of size 128x128 (so image that is read has shape (3,128,128))\n",
    "  2. a file \"labels.txt\" containing the center points (as x,y) and radius of the circles for all images in \"images\" directory\n",
    "\n",
    "Since this problem is somewhat open ended, you are given a suggested structure and some functionality that is already implemented for you (see below). You are not at all obigated to follow this structure or use the code given. \n",
    "\n",
    "**Note:** \n",
    "- \"network\" and \"model\" are used interchangeably throughout this documentation.\n",
    " \n",
    " \n",
    "### **General structure (in order):**\n",
    " 1. DataSet and DataLoader            - this is how you load data into the network **(given to you)**\n",
    " 2. Neural Network definition         - the class that defines your model   **(need to implement)**\n",
    " 3. calculate model parameters number - useful to get a feel for network's size **(given to you)**\n",
    " 4. loss function definition          - to be used in training **(need to implement)**\n",
    " 5. create an optimizer               - choose your optimizer **(need to implement)**\n",
    " 6. estimate number of ops per forward feed - the bigger this is the slower the training **(given to you)**\n",
    " 7. estimate ops per forward feed     - use function given to you to estimate this **(implement for convenience)**\n",
    " 8. view images and target and net labels - example code to help you see how to use loaders **(given to you)**\n",
    " 9. validate_model                   - returns avg loss per image for a model and loader  **(need to implement)**\n",
    " 10. train_model                      - trains the network **(need to implement)**\n",
    " 11. plot train/validaion losses      - visualizing train and validation losses from training **(given to you)**\n",
    " 12. save/load model                  - Allows you to save and load model to disk for later use **(given to you)**\n",
    " 13. visualizing images    - Painting circles prodiced by network on images from a given loader **(given to you)**\n",
    " \n",
    " \n",
    "### ** What you need to do **\n",
    "You have 5 (and a half...) things to do:\n",
    " 1. Create a CircleNet class, which is your network model. This is a key component. The output of your model should be 3 numbers that represent the the center and radius of the circle in the input image fed into your network. You should keep in mind the number of parameters in your model. If there are too many, it may overfit (and take longer to run). If there are too few, you it may not be able to learn the task needed. A typical structure would have convolutional layers first and fully connected at the end, thus reducing number of parameteres. Consider which activation function you want to use, and whether or not you wish to use batch normalization or dropout. Pooling layers are also possible. Be creative.\n",
    " \n",
    " 2. Create a loss function. This is another key component as it defines what it means for two circles (the true circle and its estimation) to be similar or not. Namely, two circles (represented by two centers and two radii: ground truth (\"labels\") and network outputs (\"outputs\")), whose images look similar should have a smaller loss than two circles whose images look less simialr. Think about how you would quantify \"closeness\"/\"similarity\" of circles.\n",
    "\n",
    " 3. Choose an optimizer. Look here for some ideas: https://pytorch.org/docs/master/optim.html\n",
    " \n",
    " 4. (This is the \"half\" thing to do). For your conveinece you may want to use the function calc_ops() that is given to you to calculate an estimate of the number of operations that your network does per feed forward. To do this, you need to enter your own network structure. An example of an arbitrary network is privided to you.\n",
    " \n",
    " 5. Create a validate_model function that assesses (tests) the performance of a model. It returns the average loss per image for a given loader. It can be run on any set of data (train, validation, or test). You may want to run this function on validation set (loader) from within the train function (see below) train after each epoch so as to see how loss behaves on validation during the training process.\n",
    " \n",
    " 6. Create a train_model function that trains model. This function updates the following parameteres: model, train_losses, and validation_losses. Model is updated simply by the training processes when optimzer.step() is called. The other two parameteres are lists that hold the average loss per image for the corresponding data (train or validation). After every epoch (i.e., iteration that goes over the entire train data), you should save to these lists the average loss per image for the corresponding data loader. These lists are useful in that you can plot them (functionality given to you) and observe how model behaves. Observe: this fucnton returs nothing, however, it updates parameteres by reference. Specifically, model is updated (trained), and so are train and validation losses values.\n",
    " \n",
    "**Note:** You are given three sets of data: trian, validation, and test. It is recommended that test not be touched until the very end, and validation be used to get a sense of your network's performance. \n",
    "\n",
    "### ** Running on a GPU: **\n",
    "It is not necessary to use a GPU for this project. However, if you choose to do so, you will gain a major speedup to your training, which will save you much time. The code given to you identifies the hardware used and will  automatically run on either a GPU or CPU. \n",
    "\n",
    "### ** Useful links: **\n",
    "1. PyTorch master tutorial - VERY useful: https://pytorch.org/docs/master/nn.html\n",
    "2. PyTorch optimizers: https://pytorch.org/docs/master/optim.html\n",
    "3. A list of possible reasons why things go wrong: https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607#74de\n",
    "\n",
    "### ** Final tips: **\n",
    "Use the Internet! Things will not work first time. You will get strange error messages. Google them up. The web is  great resource for tackling problems ranging from python error messages, to things not doing what you'd like them to do.\n",
    "\n",
    "\n",
    "### ** Submission Instructions**\n",
    "The project is to be submittd in teams as in the homework. You need to submit the following three files:\n",
    "1. model.dat                            - This is your saved model \n",
    "2. paint_that_circle.ipynb              - This is this notebook containing all of your work\n",
    "3. model.py                             - A file containing your CircleNet class only. \n",
    "\n",
    "Before you submit, run \"check_before_submission.ipynb\" to make sure your model could be properly tested. See instructions for running this notebook inside.\n",
    "\n",
    "Make sure your names appear at the top of this notebook in the appropriate place.\n",
    "\n",
    "### **GOOD LUCK!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "from PIL import ImageDraw\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset class definition (given to you) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset_dir):\n",
    "        \"\"\"\n",
    "        Initializing dataset by generating a dicitonary of labels, where an image file name is the key \n",
    "        and its labels are the contents of that entry in the dictionary. Images are not loaded. This way it\n",
    "        is possible to iterate over arbitrarily large datasets (limited by labels dicitonary fitting \n",
    "        in memory, which is not a problem in practice)\n",
    "        \n",
    "        Args:\n",
    "            dataset_dir : path to directory with images and labels. In this directory we expect to find\n",
    "                          a directory called \"images\" containing the input images, and a file called \n",
    "                          \"labels.txt\" containing desired labels (coefficients)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.labels_dict = self.gen_labels_dict()\n",
    "        self.images_keys = list(self.labels_dict)  # getting the keys of the dictionary as list\n",
    "        self.images_keys.sort()                    # sorting so as to have in alphabetical order \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_dict)\n",
    "\n",
    "    def __getitem__(self, index):    \n",
    "        \"\"\"\n",
    "        This funtion makes it possible to iterate over the ShapesDataset\n",
    "        Args:\n",
    "            index: running index of images\n",
    "            \n",
    "        Returns:\n",
    "            sample: a dicitionary with three entries:\n",
    "                    1. 'image'  contains the image\n",
    "                    2. 'labels' contains labels (coeffs) corresponding to image\n",
    "                    3. 'fname'  contains name of file (image_key) - may be useful for debugging\n",
    "        \"\"\"\n",
    "        image_key = self.images_keys[index]     # recall - key is the file name of the corresponding image\n",
    "        image = np.array(Image.open(image_key)) # image has shape: (128, 128, 3)\n",
    "        image = image/255.0                     # simple normalization - just to maintain small numbers\n",
    "        image = np.transpose(image, (2, 0, 1))  # network needs RGB channels to be first index\n",
    "        labels = self.labels_dict[image_key]\n",
    "        sample = {'image': image, 'labels': labels, 'fname':image_key}\n",
    "        \n",
    "        #return(image, labels, image_key)\n",
    "        return sample\n",
    "    \n",
    "    \n",
    "    def gen_labels_dict(self):\n",
    "        \"\"\"\n",
    "        This fucntion generates a dictionary of labels\n",
    "        \n",
    "        Returns:\n",
    "            labels_dict: the key is image file name and the value is the corresponding \n",
    "            array of labels  \n",
    "        \"\"\"\n",
    "        \n",
    "        labels_fname = self.dataset_dir + \"/labels.txt\"\n",
    "        labels_dict = {}\n",
    "        with open(labels_fname, \"r\") as inp:\n",
    "            for line in inp:\n",
    "                line = line.split('\\n')[0]                                      # remove '\\n' from end of line \n",
    "                line = line.split(',')\n",
    "                key  = self.dataset_dir + '/images/' + line[0].strip() + \".png\" # image file name is the key\n",
    "                del line[0]\n",
    "                \n",
    "                list_from_line = [float(item) for item in line]\n",
    "                labels_dict[key] = np.asarray(list_from_line, dtype=np.float32)\n",
    "                        \n",
    "        return labels_dict             \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data Loaders (given to you) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loader examples     : 1000\n",
      "validation loader examples: 100\n",
      "test loader examples      : 100\n"
     ]
    }
   ],
   "source": [
    "train_dir      = \"./train/\"\n",
    "validation_dir = \"./validation/\"\n",
    "test_dir       = \"./test/\"\n",
    "\n",
    "\n",
    "train_dataset = ShapesDataset(train_dir)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=32,\n",
    "                          shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "validation_dataset = ShapesDataset(validation_dir)\n",
    "\n",
    "validation_loader = DataLoader(validation_dataset, \n",
    "                               batch_size=1,\n",
    "                               shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "test_dataset = ShapesDataset(test_dir)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                          batch_size=1,\n",
    "                          shuffle=False)\n",
    "\n",
    "\n",
    "print(\"train loader examples     :\", len(train_dataset)) \n",
    "print(\"validation loader examples:\", len(validation_dataset))\n",
    "print(\"test loader examples      :\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F  # a lower level (compared to torch.nn) interface\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class CircleNet(nn.Module):    # nn.Module is parent class  \n",
    "    def __init__(self):\n",
    "        super(CircleNet, self).__init__()  #calls init of parent class\n",
    "                \n",
    "\n",
    "        #----------------------------------------------\n",
    "        # implementation needed here \n",
    "        #----------------------------------------------\n",
    "      \n",
    "        \n",
    "        # The convolution layers were chosen to keep dimensions of input image: (I-F+2P)/S +1= (128-3+2)/1 + 1 = 128\n",
    "        \n",
    "        # Our images are RGB, so input channels = 3. Use 12 filters for first 2 convolution layers, then double\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=24, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        \n",
    "        \n",
    "        #Pooling to reduce sizes, and dropout to prevent overfitting\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.drop = nn.Dropout2d(p=0.2)\n",
    "#         self.norm1 = nn.BatchNorm2d(12)\n",
    "#         self.norm2 = nn.BatchNorm2d(24)\n",
    "        \n",
    "        # There are 2 pooling layers, each with kernel size of 2. Output size: 128/(2*2) = 32\n",
    "        # Feature tensors are now 32 x 32; With 24 output channels, this gives 32x32x24\n",
    "        \n",
    "        # Have 3 output features, corresponding to x-pos, y-pos, radius. \n",
    "        self.fc = nn.Linear(in_features=32 * 32 * 32, out_features=3)\n",
    "    \n",
    "\n",
    "                \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Feed forward through network\n",
    "        Args:\n",
    "            x - input to the network\n",
    "            \n",
    "        Returns \"out\", which is the network's output\n",
    "        \"\"\"\n",
    "        \n",
    "        #----------------------------------------------\n",
    "        # implementation needed here \n",
    "        #----------------------------------------------\n",
    "        \n",
    "        #Convolution 1\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.pool(out)\n",
    "        \n",
    "#         out = self.norm1(out)\n",
    "\n",
    "        #Convolution 2\n",
    "        out = self.conv2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.pool(out)\n",
    "        \n",
    "#         out = self.norm1(out)\n",
    "        \n",
    "        #Convolution 3\n",
    "        out = self.conv3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.drop(out)\n",
    "#         out = self.norm2(out)\n",
    "        \n",
    "        #Convolution 4\n",
    "        out = self.conv4(out)\n",
    "        out = self.relu(out)\n",
    "        out = F.dropout(out, training=self.training)\n",
    "        \n",
    "        \n",
    "        out = out.view(-1, 32 * 32 * 32)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        \n",
    "        return out\n",
    "             \n",
    "             \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss(outputs, labels):\n",
    "    \n",
    "    \"\"\"\n",
    "    Args:\n",
    "        outputs - output of network ([batch size, 3]) \n",
    "        labels  - desired labels  ([batch size, 3])\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = torch.zeros(1, dtype=torch.float, requires_grad=True)\n",
    "    loss = loss.to(device)\n",
    "    \n",
    "    #----------------------------------------------\n",
    "    # implementation needed here \n",
    "    #----------------------------------------------\n",
    "\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Observe: If you need to iterate and add certain values to loss defined above\n",
    "    # you cannot write: loss +=... because this will raise the error: \n",
    "    # \"Leaf variable was used in an inplace operation\"\n",
    "    # Instead, to avoid this error write: loss = loss + ...  \n",
    "    \n",
    "                                      \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get number of trainable parameters (given to you)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_params_num(model):\n",
    "    \"\"\"\n",
    "    This fucntion returns the number of trainable parameters of neural network model\n",
    "    You may want to call it after you create your model to see how many parameteres the model has\n",
    "    Args:\n",
    "        model - neural net to examine\n",
    "    \"\"\"\n",
    "    \n",
    "    #filter given iterable with a function that tests each element in the iterable to be true or not\n",
    "    model_parameters = filter(lambda p: p.requires_grad == True, model.parameters()) \n",
    "    params_num = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return params_num\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model creation and choice of optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model trainable parameters: 109511\n",
      "\n",
      "Model:\n",
      "CircleNet(\n",
      "  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(24, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu): ReLU()\n",
      "  (drop): Dropout2d(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=32768, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CircleNet().to(device)\n",
    "print (\"Number of model trainable parameters:\", get_train_params_num(model))\n",
    "\n",
    "print()\n",
    "print(\"Model:\")\n",
    "print(model)  #Used to see model to more easily plug into calc_ops()\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------------------------\n",
    "#  Choose your optimizer:\n",
    "#  implementation needed here \n",
    "#----------------------------------------------\n",
    "# Use an \"Adam\" optimizer to adjust weights\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay =0.001)\n",
    "\n",
    "\n",
    "\n",
    "####\n",
    "criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get an estimate number of operations (given to you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ops(inp_size, net_struct):\n",
    "    \"\"\"\n",
    "    Calculates a rough number of operations for a given network topology\n",
    "    Args:\n",
    "        inp_size - (W,H) of input \n",
    "        net_struct - list of tuples describing structure of network. \n",
    "        \n",
    "        Example:\n",
    "         (('conv2d', (3, 8, 3, 1, 0)),  # cin, cout, kernel, stride, pad\n",
    "          ('conv2d': 83, 8, 3, 1, 0)),\n",
    "          ('MaxPool2d', (2,2)),         # kernel, stride\n",
    "          ('fc': (64, 8)),      \n",
    "          ('fc': (8, 4)))\n",
    "         \n",
    "    \"\"\"\n",
    "    \n",
    "    ops = 0\n",
    "    W, H = inp_size\n",
    "    for curr_item in net_struct:\n",
    "        if curr_item[0] == 'conv2d':\n",
    "            cin = curr_item[1][0]\n",
    "            cout = curr_item[1][1]\n",
    "            kernel = curr_item[1][2]\n",
    "            stride = curr_item[1][3]\n",
    "            pad = curr_item[1][4]\n",
    "            W = (W +2*pad - kernel)/stride + 1\n",
    "            H = (H +2*pad - kernel)/stride + 1\n",
    "            curr_ops = (W*H*cin*cout*kernel*kernel)/stride\n",
    "            ops += curr_ops\n",
    "            print (curr_item, \":\",  \"{:,}\".format(int(curr_ops)))\n",
    "        elif curr_item[0] == 'MaxPool2d':\n",
    "            kernel = curr_item[1][0]\n",
    "            stride = curr_item[1][1]\n",
    "            W = (W - kernel)/stride + 1\n",
    "            H = (H - kernel)/stride + 1\n",
    "        else:\n",
    "            curr_ops = curr_item[1][0] * curr_item[1][1]\n",
    "            ops += curr_ops\n",
    "            print (curr_item, \":\",  \"{:,}\".format(int(curr_ops)))\n",
    "            \n",
    "    return int(ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check rough number of ops for network (for your convenience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('conv2d', (3, 3, 3, 1, 1)) : 1,327,104\n",
      "('conv2d', (3, 3, 3, 1, 1)) : 331,776\n",
      "('fc', (2883, 4)) : 11,532\n",
      "\n",
      "Example Total ops: 1,670,412\n",
      "('conv1', (3, 12, 3, 1, 1)) : 36\n",
      "('conv2', (12, 12, 3, 1, 1)) : 144\n",
      "('conv3', (12, 24, 3, 1, 1)) : 288\n",
      "('pool', (2, 2)) : 4\n",
      "('fc', (24576, 3)) : 73,728\n",
      "\n",
      "CircleNet Total ops: 74,200\n"
     ]
    }
   ],
   "source": [
    "inp_size = (128,128)\n",
    "\n",
    "# place your network ropology in example_net below to obtain an estimated number of operations for your network\n",
    "example_net = (('conv2d', (3, 3, 3, 1, 1)),\n",
    "               ('MaxPool2d', (2,2)),\n",
    "               ('conv2d', (3, 3, 3, 1, 1)),\n",
    "               ('MaxPool2d', (2,2)),\n",
    "               ('fc', (2883, 4)))\n",
    "\n",
    "\n",
    "circlenet1 = (('conv1', (3,12,3,1,1)),\n",
    "          ('conv2',(12,12,3,1,1)),\n",
    "          ('conv3', (12,24,3,1,1)),\n",
    "          ('pool', (2,2)), \n",
    "          ('fc', (24576,3)))\n",
    "\n",
    "\n",
    "\n",
    "ops = calc_ops(inp_size, example_net)\n",
    "print()\n",
    "print(\"Example Total ops: {:,}\".format(ops))\n",
    "\n",
    "num_ops_circlenet1 = calc_ops(inp_size, circlenet1)\n",
    "print()\n",
    "print(\"CircleNet Total ops: {:,}\".format(num_ops_circlenet1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View images, target circle labels and  network outputs (given to you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing image:  ./train//images/0828.png\n",
      "Target labels : [0.38, 0.81, 0.18]\n",
      "\n",
      "showing image:  ./train//images/0956.png\n",
      "Target labels : [0.52, 0.32, 0.2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "View first image of a given number of batches assuming that model has been created. \n",
    "Currently, lines assuming model has been creatd, are commented out. Without a model, \n",
    "you can view target labels and the corresponding images.\n",
    "This is given to you so that you may see how loaders and model can be used. \n",
    "\"\"\"\n",
    "\n",
    "loader = train_loader # choose from which loader to show images\n",
    "bacthes_to_show = 2\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(loader, 0): #0 means that counting starts at zero\n",
    "        inputs = (data['image']).to(device)   # has shape (batch_size, 3, 128, 128)\n",
    "        labels = (data['labels']).to(device)  # has shape (batch_size, 3)\n",
    "        img_fnames = data['fname']            # list of length batch_size\n",
    "        \n",
    "        #outputs = model(inputs.float())\n",
    "        img = Image.open(img_fnames[0])\n",
    "        \n",
    "        print (\"showing image: \", img_fnames[0])\n",
    "        \n",
    "        labels_str = [ float((\"{0:.2f}\".format(x))) for x in labels[0]]#labels_np_arr]\n",
    "        \n",
    "        #outputs_np_arr = outputs[0] # using \".numpy()\" to convert tensor to numpy array\n",
    "        #outputs_str = [ float((\"{0:.2f}\".format(x))) for x in outputs_np_arr]\n",
    "        print(\"Target labels :\", labels_str )\n",
    "        #print(\"network coeffs:\", outputs_str)\n",
    "        print()\n",
    "        #img.show()\n",
    "        \n",
    "        if (i+1) == bacthes_to_show:\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, loader):\n",
    "    \"\"\"\n",
    "    This function parses a given loader and returns the avergae (per image) loss \n",
    "    (as defined by \"my_loss\") of the entire dataset associated with the given loader.\n",
    "    \n",
    "    Args:\n",
    "        model  - neural network to examine\n",
    "        loader - where input data comes from (train, validation, or test)\n",
    "        \n",
    "    returns:\n",
    "        average loss per image in variable named \"avg_loss\"\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "                  # (dropout is set to zero)\n",
    "\n",
    "    #----------------------------------------------\n",
    "    # implementation needed here \n",
    "    #----------------------------------------------\n",
    "  \n",
    "    # return average loss for the epoch\n",
    "    # Switch the model to evaluation mode (so we don't backpropagate or drop)\n",
    "    model.eval()\n",
    "#     validation_loss = 0\n",
    "    total_validation_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_id, data in enumerate(loader):\n",
    "            inputs = (data['image']).to(device)   # has shape (batch_size, 3, 128, 128)\n",
    "            labels = (data['labels']).to(device)  # has shape (batch_size, 3)\n",
    "            img_fnames = data['fname']            # list of length batch_size\n",
    "            \n",
    "#             data, target = data.to(device), target.to(device)\n",
    "            output = model(inputs.float())\n",
    "            # loss for current batch\n",
    "            loss = my_loss(output, labels)\n",
    "            total_validation_loss = total_validation_loss + loss.item()\n",
    "            \n",
    "            \n",
    "            # Calculate the loss for this batch and add to running total\n",
    "#             validation_loss += my_loss(output, labels).item()\n",
    "            \n",
    "\n",
    "        avg_loss = total_validation_loss / (batch_id+1)\n",
    "    \n",
    "    model.train()  #back to default\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,\n",
    "                optimizer,\n",
    "                train_loader,\n",
    "                validation_loader,\n",
    "                train_losses,\n",
    "                validation_losses,\n",
    "                epochs=1):\n",
    "    \n",
    "    \"\"\"\n",
    "    Trains a neural network. \n",
    "    Args:\n",
    "        model               - model to be trained\n",
    "        optimizer           - optimizer used for training\n",
    "        train_loader        - loader from which data for training comes \n",
    "        validation_loader   - loader from which data for validation comes (maybe at the end, you use test_loader)\n",
    "        train_losses        - adding train loss value to this list for future analysis\n",
    "        validation_losses   - adding validation loss value to this list for future analysis\n",
    "        epochs              - number of runs over the entire data set \n",
    "    \"\"\"\n",
    "    \n",
    "    #----------------------------------------------\n",
    "    # implementation needed here \n",
    "    #----------------------------------------------\n",
    "  \n",
    "    #train_loss = 0\n",
    "    # model.train()  # set to training mode\n",
    "    \n",
    "    print(\"Epoch:\", epochs)\n",
    "    for ep in range(epochs):      #each epoch loops over entire dataset\n",
    "        print(\"\\nEpoch # \", ep+1)\n",
    "        total_training_loss = 0    #RESET THE LOSS FOR NEW EPOCH!!!!\n",
    "        for batch_id, data in enumerate(train_loader, 0): #0 means that counting starts at zero\n",
    "            inputs = (data['image']).to(device)   # has shape (batch_size, 3, 128, 128)\n",
    "            labels = (data['labels']).to(device)  # has shape (batch_size, 3)\n",
    "            img_fnames = data['fname']            # list of length batch_size   \n",
    "            \n",
    "\n",
    "            #reset the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #pass data through model\n",
    "            output = model(inputs.float())\n",
    "            \n",
    "            # get loss for current batch\n",
    "            loss = my_loss(output, labels)\n",
    "            # Backpropagate\n",
    "            loss.backward()    \n",
    "            optimizer.step()\n",
    "            \n",
    "            #train_loss is a running total of loss over all batches so far\n",
    "            total_training_loss = total_training_loss + loss.item()\n",
    "            \n",
    "            \n",
    "            \n",
    "            images_counted = ((batch_id+1) * 32)\n",
    "            print('\\tTraining batch {}, Images: {}/1000 Total_loss: {}, Loss: {:.6f}'.format(batch_id + 1,images_counted, total_training_loss, loss.item()))\n",
    "            \n",
    "        \n",
    "        #average loss for each batch: \n",
    "        avg_loss = total_training_loss / (batch_id+1)\n",
    "        train_losses.append(avg_loss)   \n",
    "            \n",
    "        print(\"Average loss for Training set {}: {:.6f}\".format(ep+1,avg_loss))\n",
    "        validation_avg_loss = validate_model(model, validation_loader)\n",
    "        print(\"Average loss for Validation set {}: {:.6f}\".format(ep+1,validation_avg_loss))\n",
    "        validation_losses.append(validation_avg_loss)\n",
    "    \n",
    "\n",
    "    return \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual train (given to you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30\n",
      "\n",
      "Epoch #  1\n",
      "\tTraining batch 1, Images: 32/1000 Total_loss: 0.20647895336151123, Loss: 0.206479\n",
      "\tTraining batch 2, Images: 64/1000 Total_loss: 1.4576423168182373, Loss: 1.251163\n",
      "\tTraining batch 3, Images: 96/1000 Total_loss: 1.512425683438778, Loss: 0.054783\n",
      "\tTraining batch 4, Images: 128/1000 Total_loss: 1.6254053339362144, Loss: 0.112980\n",
      "\tTraining batch 5, Images: 160/1000 Total_loss: 1.8841088935732841, Loss: 0.258704\n",
      "\tTraining batch 6, Images: 192/1000 Total_loss: 2.169802747666836, Loss: 0.285694\n",
      "\tTraining batch 7, Images: 224/1000 Total_loss: 2.3352193757891655, Loss: 0.165417\n",
      "\tTraining batch 8, Images: 256/1000 Total_loss: 2.4324485138058662, Loss: 0.097229\n",
      "\tTraining batch 9, Images: 288/1000 Total_loss: 2.473510518670082, Loss: 0.041062\n",
      "\tTraining batch 10, Images: 320/1000 Total_loss: 2.5133952647447586, Loss: 0.039885\n",
      "\tTraining batch 11, Images: 352/1000 Total_loss: 2.596932530403137, Loss: 0.083537\n",
      "\tTraining batch 12, Images: 384/1000 Total_loss: 2.697964869439602, Loss: 0.101032\n",
      "\tTraining batch 13, Images: 416/1000 Total_loss: 2.7810467183589935, Loss: 0.083082\n",
      "\tTraining batch 14, Images: 448/1000 Total_loss: 2.818132109940052, Loss: 0.037085\n",
      "\tTraining batch 15, Images: 480/1000 Total_loss: 2.8455922212451696, Loss: 0.027460\n",
      "\tTraining batch 16, Images: 512/1000 Total_loss: 2.873691128566861, Loss: 0.028099\n",
      "\tTraining batch 17, Images: 544/1000 Total_loss: 2.919800804927945, Loss: 0.046110\n",
      "\tTraining batch 18, Images: 576/1000 Total_loss: 2.9663555044680834, Loss: 0.046555\n",
      "\tTraining batch 19, Images: 608/1000 Total_loss: 3.019732726737857, Loss: 0.053377\n",
      "\tTraining batch 20, Images: 640/1000 Total_loss: 3.065840108320117, Loss: 0.046107\n",
      "\tTraining batch 21, Images: 672/1000 Total_loss: 3.1013745423406363, Loss: 0.035534\n",
      "\tTraining batch 22, Images: 704/1000 Total_loss: 3.1371924486011267, Loss: 0.035818\n",
      "\tTraining batch 23, Images: 736/1000 Total_loss: 3.1663784626871347, Loss: 0.029186\n",
      "\tTraining batch 24, Images: 768/1000 Total_loss: 3.1967967804521322, Loss: 0.030418\n",
      "\tTraining batch 25, Images: 800/1000 Total_loss: 3.2243109364062548, Loss: 0.027514\n",
      "\tTraining batch 26, Images: 832/1000 Total_loss: 3.248576946556568, Loss: 0.024266\n",
      "\tTraining batch 27, Images: 864/1000 Total_loss: 3.28291305154562, Loss: 0.034336\n",
      "\tTraining batch 28, Images: 896/1000 Total_loss: 3.3100358210504055, Loss: 0.027123\n",
      "\tTraining batch 29, Images: 928/1000 Total_loss: 3.338894123211503, Loss: 0.028858\n",
      "\tTraining batch 30, Images: 960/1000 Total_loss: 3.3716329727321863, Loss: 0.032739\n",
      "\tTraining batch 31, Images: 992/1000 Total_loss: 3.3994360715150833, Loss: 0.027803\n",
      "\tTraining batch 32, Images: 1024/1000 Total_loss: 3.4212578125298023, Loss: 0.021822\n",
      "Average loss for Training set 1: 0.106914\n",
      "Average loss for Validation set 1: 0.021761\n",
      "\n",
      "Epoch #  2\n",
      "\tTraining batch 1, Images: 32/1000 Total_loss: 0.021470826119184494, Loss: 0.021471\n",
      "\tTraining batch 2, Images: 64/1000 Total_loss: 0.04618374444544315, Loss: 0.024713\n",
      "\tTraining batch 3, Images: 96/1000 Total_loss: 0.07081350684165955, Loss: 0.024630\n",
      "\tTraining batch 4, Images: 128/1000 Total_loss: 0.09933512471616268, Loss: 0.028522\n",
      "\tTraining batch 5, Images: 160/1000 Total_loss: 0.1273199561983347, Loss: 0.027985\n",
      "\tTraining batch 6, Images: 192/1000 Total_loss: 0.15670320391654968, Loss: 0.029383\n",
      "\tTraining batch 7, Images: 224/1000 Total_loss: 0.17721055075526237, Loss: 0.020507\n",
      "\tTraining batch 8, Images: 256/1000 Total_loss: 0.20144471898674965, Loss: 0.024234\n",
      "\tTraining batch 9, Images: 288/1000 Total_loss: 0.2263239249587059, Loss: 0.024879\n",
      "\tTraining batch 10, Images: 320/1000 Total_loss: 0.25019023194909096, Loss: 0.023866\n",
      "\tTraining batch 11, Images: 352/1000 Total_loss: 0.27693168818950653, Loss: 0.026741\n",
      "\tTraining batch 12, Images: 384/1000 Total_loss: 0.30217123590409756, Loss: 0.025240\n",
      "\tTraining batch 13, Images: 416/1000 Total_loss: 0.32384814508259296, Loss: 0.021677\n",
      "\tTraining batch 14, Images: 448/1000 Total_loss: 0.3437176551669836, Loss: 0.019870\n",
      "\tTraining batch 15, Images: 480/1000 Total_loss: 0.3632331807166338, Loss: 0.019516\n",
      "\tTraining batch 16, Images: 512/1000 Total_loss: 0.3833812680095434, Loss: 0.020148\n",
      "\tTraining batch 17, Images: 544/1000 Total_loss: 0.4074361678212881, Loss: 0.024055\n",
      "\tTraining batch 18, Images: 576/1000 Total_loss: 0.4314772244542837, Loss: 0.024041\n",
      "\tTraining batch 19, Images: 608/1000 Total_loss: 0.44963688403367996, Loss: 0.018160\n",
      "\tTraining batch 20, Images: 640/1000 Total_loss: 0.4739120714366436, Loss: 0.024275\n",
      "\tTraining batch 21, Images: 672/1000 Total_loss: 0.49512348510324955, Loss: 0.021211\n",
      "\tTraining batch 22, Images: 704/1000 Total_loss: 0.5198131948709488, Loss: 0.024690\n",
      "\tTraining batch 23, Images: 736/1000 Total_loss: 0.5418114215135574, Loss: 0.021998\n",
      "\tTraining batch 24, Images: 768/1000 Total_loss: 0.5630481354892254, Loss: 0.021237\n",
      "\tTraining batch 25, Images: 800/1000 Total_loss: 0.5843789149075747, Loss: 0.021331\n",
      "\tTraining batch 26, Images: 832/1000 Total_loss: 0.6036702785640955, Loss: 0.019291\n",
      "\tTraining batch 27, Images: 864/1000 Total_loss: 0.6237679924815893, Loss: 0.020098\n",
      "\tTraining batch 28, Images: 896/1000 Total_loss: 0.646299609914422, Loss: 0.022532\n",
      "\tTraining batch 29, Images: 928/1000 Total_loss: 0.6658198218792677, Loss: 0.019520\n",
      "\tTraining batch 30, Images: 960/1000 Total_loss: 0.6858407780528069, Loss: 0.020021\n",
      "\tTraining batch 31, Images: 992/1000 Total_loss: 0.7031864505261183, Loss: 0.017346\n",
      "\tTraining batch 32, Images: 1024/1000 Total_loss: 0.7326142601668835, Loss: 0.029428\n",
      "Average loss for Training set 2: 0.022894\n",
      "Average loss for Validation set 2: 0.022501\n",
      "\n",
      "Epoch #  3\n",
      "\tTraining batch 1, Images: 32/1000 Total_loss: 0.022249579429626465, Loss: 0.022250\n",
      "\tTraining batch 2, Images: 64/1000 Total_loss: 0.04456326737999916, Loss: 0.022314\n",
      "\tTraining batch 3, Images: 96/1000 Total_loss: 0.07043149508535862, Loss: 0.025868\n",
      "\tTraining batch 4, Images: 128/1000 Total_loss: 0.09572366066277027, Loss: 0.025292\n",
      "\tTraining batch 5, Images: 160/1000 Total_loss: 0.11963192373514175, Loss: 0.023908\n",
      "\tTraining batch 6, Images: 192/1000 Total_loss: 0.1441469918936491, Loss: 0.024515\n",
      "\tTraining batch 7, Images: 224/1000 Total_loss: 0.16596454568207264, Loss: 0.021818\n",
      "\tTraining batch 8, Images: 256/1000 Total_loss: 0.18379807844758034, Loss: 0.017834\n",
      "\tTraining batch 9, Images: 288/1000 Total_loss: 0.20428162068128586, Loss: 0.020484\n",
      "\tTraining batch 10, Images: 320/1000 Total_loss: 0.2260216660797596, Loss: 0.021740\n",
      "\tTraining batch 11, Images: 352/1000 Total_loss: 0.25069836899638176, Loss: 0.024677\n",
      "\tTraining batch 12, Images: 384/1000 Total_loss: 0.2731705028563738, Loss: 0.022472\n",
      "\tTraining batch 13, Images: 416/1000 Total_loss: 0.2967481631785631, Loss: 0.023578\n",
      "\tTraining batch 14, Images: 448/1000 Total_loss: 0.3166852705180645, Loss: 0.019937\n",
      "\tTraining batch 15, Images: 480/1000 Total_loss: 0.3376549407839775, Loss: 0.020970\n",
      "\tTraining batch 16, Images: 512/1000 Total_loss: 0.3611883409321308, Loss: 0.023533\n",
      "\tTraining batch 17, Images: 544/1000 Total_loss: 0.3838012907654047, Loss: 0.022613\n",
      "\tTraining batch 18, Images: 576/1000 Total_loss: 0.40867034532129765, Loss: 0.024869\n",
      "\tTraining batch 19, Images: 608/1000 Total_loss: 0.4295739624649286, Loss: 0.020904\n",
      "\tTraining batch 20, Images: 640/1000 Total_loss: 0.4520463962107897, Loss: 0.022472\n",
      "\tTraining batch 21, Images: 672/1000 Total_loss: 0.46773850731551647, Loss: 0.015692\n",
      "\tTraining batch 22, Images: 704/1000 Total_loss: 0.4905605260282755, Loss: 0.022822\n",
      "\tTraining batch 23, Images: 736/1000 Total_loss: 0.5101428162306547, Loss: 0.019582\n",
      "\tTraining batch 24, Images: 768/1000 Total_loss: 0.5259980224072933, Loss: 0.015855\n",
      "\tTraining batch 25, Images: 800/1000 Total_loss: 0.5465630255639553, Loss: 0.020565\n",
      "\tTraining batch 26, Images: 832/1000 Total_loss: 0.5642221048474312, Loss: 0.017659\n",
      "\tTraining batch 27, Images: 864/1000 Total_loss: 0.5873857345432043, Loss: 0.023164\n",
      "\tTraining batch 28, Images: 896/1000 Total_loss: 0.6069276630878448, Loss: 0.019542\n",
      "\tTraining batch 29, Images: 928/1000 Total_loss: 0.632179394364357, Loss: 0.025252\n",
      "\tTraining batch 30, Images: 960/1000 Total_loss: 0.6549868024885654, Loss: 0.022807\n",
      "\tTraining batch 31, Images: 992/1000 Total_loss: 0.6769483089447021, Loss: 0.021962\n",
      "\tTraining batch 32, Images: 1024/1000 Total_loss: 0.697166420519352, Loss: 0.020218\n",
      "Average loss for Training set 3: 0.021786\n",
      "Average loss for Validation set 3: 0.020775\n",
      "\n",
      "Epoch #  4\n",
      "\tTraining batch 1, Images: 32/1000 Total_loss: 0.022550538182258606, Loss: 0.022551\n",
      "\tTraining batch 2, Images: 64/1000 Total_loss: 0.04537888988852501, Loss: 0.022828\n",
      "\tTraining batch 3, Images: 96/1000 Total_loss: 0.06788923032581806, Loss: 0.022510\n",
      "\tTraining batch 4, Images: 128/1000 Total_loss: 0.08995642513036728, Loss: 0.022067\n",
      "\tTraining batch 5, Images: 160/1000 Total_loss: 0.10995959118008614, Loss: 0.020003\n",
      "\tTraining batch 6, Images: 192/1000 Total_loss: 0.13447859324514866, Loss: 0.024519\n",
      "\tTraining batch 7, Images: 224/1000 Total_loss: 0.15348570235073566, Loss: 0.019007\n",
      "\tTraining batch 8, Images: 256/1000 Total_loss: 0.17720739729702473, Loss: 0.023722\n",
      "\tTraining batch 9, Images: 288/1000 Total_loss: 0.19768975488841534, Loss: 0.020482\n",
      "\tTraining batch 10, Images: 320/1000 Total_loss: 0.21984372101724148, Loss: 0.022154\n",
      "\tTraining batch 11, Images: 352/1000 Total_loss: 0.24117358960211277, Loss: 0.021330\n",
      "\tTraining batch 12, Images: 384/1000 Total_loss: 0.2649488989263773, Loss: 0.023775\n",
      "\tTraining batch 13, Images: 416/1000 Total_loss: 0.28568179346621037, Loss: 0.020733\n",
      "\tTraining batch 14, Images: 448/1000 Total_loss: 0.30944082140922546, Loss: 0.023759\n",
      "\tTraining batch 15, Images: 480/1000 Total_loss: 0.3332163915038109, Loss: 0.023776\n",
      "\tTraining batch 16, Images: 512/1000 Total_loss: 0.35322258807718754, Loss: 0.020006\n",
      "\tTraining batch 17, Images: 544/1000 Total_loss: 0.3726691398769617, Loss: 0.019447\n",
      "\tTraining batch 18, Images: 576/1000 Total_loss: 0.3968440443277359, Loss: 0.024175\n",
      "\tTraining batch 19, Images: 608/1000 Total_loss: 0.41956643760204315, Loss: 0.022722\n",
      "\tTraining batch 20, Images: 640/1000 Total_loss: 0.43955014273524284, Loss: 0.019984\n",
      "\tTraining batch 21, Images: 672/1000 Total_loss: 0.4640374295413494, Loss: 0.024487\n",
      "\tTraining batch 22, Images: 704/1000 Total_loss: 0.48806924372911453, Loss: 0.024032\n",
      "\tTraining batch 23, Images: 736/1000 Total_loss: 0.5120300762355328, Loss: 0.023961\n",
      "\tTraining batch 24, Images: 768/1000 Total_loss: 0.534542778506875, Loss: 0.022513\n",
      "\tTraining batch 25, Images: 800/1000 Total_loss: 0.555595925077796, Loss: 0.021053\n",
      "\tTraining batch 26, Images: 832/1000 Total_loss: 0.5744595993310213, Loss: 0.018864\n",
      "\tTraining batch 27, Images: 864/1000 Total_loss: 0.5991691295057535, Loss: 0.024710\n",
      "\tTraining batch 28, Images: 896/1000 Total_loss: 0.6183252744376659, Loss: 0.019156\n",
      "\tTraining batch 29, Images: 928/1000 Total_loss: 0.6380196157842875, Loss: 0.019694\n",
      "\tTraining batch 30, Images: 960/1000 Total_loss: 0.6593980174511671, Loss: 0.021378\n",
      "\tTraining batch 31, Images: 992/1000 Total_loss: 0.684696463868022, Loss: 0.025298\n",
      "\tTraining batch 32, Images: 1024/1000 Total_loss: 0.7102312184870243, Loss: 0.025535\n",
      "Average loss for Training set 4: 0.022195\n",
      "Average loss for Validation set 4: 0.021438\n",
      "\n",
      "Epoch #  5\n",
      "\tTraining batch 1, Images: 32/1000 Total_loss: 0.021216779947280884, Loss: 0.021217\n",
      "\tTraining batch 2, Images: 64/1000 Total_loss: 0.04568292945623398, Loss: 0.024466\n",
      "\tTraining batch 3, Images: 96/1000 Total_loss: 0.06323155201971531, Loss: 0.017549\n",
      "\tTraining batch 4, Images: 128/1000 Total_loss: 0.08693702705204487, Loss: 0.023705\n",
      "\tTraining batch 5, Images: 160/1000 Total_loss: 0.1110438834875822, Loss: 0.024107\n",
      "\tTraining batch 6, Images: 192/1000 Total_loss: 0.13183408416807652, Loss: 0.020790\n",
      "\tTraining batch 7, Images: 224/1000 Total_loss: 0.15449609607458115, Loss: 0.022662\n",
      "\tTraining batch 8, Images: 256/1000 Total_loss: 0.17549562454223633, Loss: 0.021000\n",
      "\tTraining batch 9, Images: 288/1000 Total_loss: 0.20173023268580437, Loss: 0.026235\n",
      "\tTraining batch 10, Images: 320/1000 Total_loss: 0.222889157012105, Loss: 0.021159\n",
      "\tTraining batch 11, Images: 352/1000 Total_loss: 0.24233422428369522, Loss: 0.019445\n",
      "\tTraining batch 12, Images: 384/1000 Total_loss: 0.2628719713538885, Loss: 0.020538\n",
      "\tTraining batch 13, Images: 416/1000 Total_loss: 0.2896246425807476, Loss: 0.026753\n",
      "\tTraining batch 14, Images: 448/1000 Total_loss: 0.31342821195721626, Loss: 0.023804\n",
      "\tTraining batch 15, Images: 480/1000 Total_loss: 0.3342857304960489, Loss: 0.020858\n",
      "\tTraining batch 16, Images: 512/1000 Total_loss: 0.3530026152729988, Loss: 0.018717\n",
      "\tTraining batch 17, Images: 544/1000 Total_loss: 0.37414034083485603, Loss: 0.021138\n",
      "\tTraining batch 18, Images: 576/1000 Total_loss: 0.39677736535668373, Loss: 0.022637\n",
      "\tTraining batch 19, Images: 608/1000 Total_loss: 0.4171036556363106, Loss: 0.020326\n",
      "\tTraining batch 20, Images: 640/1000 Total_loss: 0.4404160063713789, Loss: 0.023312\n",
      "\tTraining batch 21, Images: 672/1000 Total_loss: 0.46057184413075447, Loss: 0.020156\n",
      "\tTraining batch 22, Images: 704/1000 Total_loss: 0.4810328893363476, Loss: 0.020461\n",
      "\tTraining batch 23, Images: 736/1000 Total_loss: 0.5022230911999941, Loss: 0.021190\n",
      "\tTraining batch 24, Images: 768/1000 Total_loss: 0.5241286437958479, Loss: 0.021906\n",
      "\tTraining batch 25, Images: 800/1000 Total_loss: 0.5474624820053577, Loss: 0.023334\n",
      "\tTraining batch 26, Images: 832/1000 Total_loss: 0.5692352913320065, Loss: 0.021773\n",
      "\tTraining batch 27, Images: 864/1000 Total_loss: 0.586037989705801, Loss: 0.016803\n",
      "\tTraining batch 28, Images: 896/1000 Total_loss: 0.6070356853306293, Loss: 0.020998\n",
      "\tTraining batch 29, Images: 928/1000 Total_loss: 0.6299279220402241, Loss: 0.022892\n",
      "\tTraining batch 30, Images: 960/1000 Total_loss: 0.6484378911554813, Loss: 0.018510\n",
      "\tTraining batch 31, Images: 992/1000 Total_loss: 0.6687474586069584, Loss: 0.020310\n",
      "\tTraining batch 32, Images: 1024/1000 Total_loss: 0.6908324956893921, Loss: 0.022085\n",
      "Average loss for Training set 5: 0.021589\n",
      "Average loss for Validation set 5: 0.021526\n",
      "\n",
      "Epoch #  6\n",
      "\tTraining batch 1, Images: 32/1000 Total_loss: 0.02604239247739315, Loss: 0.026042\n",
      "\tTraining batch 2, Images: 64/1000 Total_loss: 0.04487921670079231, Loss: 0.018837\n",
      "\tTraining batch 3, Images: 96/1000 Total_loss: 0.06895171478390694, Loss: 0.024072\n",
      "\tTraining batch 4, Images: 128/1000 Total_loss: 0.09099428355693817, Loss: 0.022043\n",
      "\tTraining batch 5, Images: 160/1000 Total_loss: 0.11237849108874798, Loss: 0.021384\n",
      "\tTraining batch 6, Images: 192/1000 Total_loss: 0.13337954692542553, Loss: 0.021001\n",
      "\tTraining batch 7, Images: 224/1000 Total_loss: 0.15777547284960747, Loss: 0.024396\n",
      "\tTraining batch 8, Images: 256/1000 Total_loss: 0.17432638630270958, Loss: 0.016551\n",
      "\tTraining batch 9, Images: 288/1000 Total_loss: 0.19607605412602425, Loss: 0.021750\n",
      "\tTraining batch 10, Images: 320/1000 Total_loss: 0.21556438878178596, Loss: 0.019488\n",
      "\tTraining batch 11, Images: 352/1000 Total_loss: 0.23857533372938633, Loss: 0.023011\n",
      "\tTraining batch 12, Images: 384/1000 Total_loss: 0.2597956359386444, Loss: 0.021220\n",
      "\tTraining batch 13, Images: 416/1000 Total_loss: 0.28332145512104034, Loss: 0.023526\n",
      "\tTraining batch 14, Images: 448/1000 Total_loss: 0.3057028613984585, Loss: 0.022381\n",
      "\tTraining batch 15, Images: 480/1000 Total_loss: 0.3239586390554905, Loss: 0.018256\n",
      "\tTraining batch 16, Images: 512/1000 Total_loss: 0.34820567443966866, Loss: 0.024247\n",
      "\tTraining batch 17, Images: 544/1000 Total_loss: 0.36615270003676414, Loss: 0.017947\n",
      "\tTraining batch 18, Images: 576/1000 Total_loss: 0.3895900249481201, Loss: 0.023437\n",
      "\tTraining batch 19, Images: 608/1000 Total_loss: 0.41117786429822445, Loss: 0.021588\n",
      "\tTraining batch 20, Images: 640/1000 Total_loss: 0.42684523202478886, Loss: 0.015667\n",
      "\tTraining batch 21, Images: 672/1000 Total_loss: 0.44514005444943905, Loss: 0.018295\n",
      "\tTraining batch 22, Images: 704/1000 Total_loss: 0.46522315219044685, Loss: 0.020083\n",
      "\tTraining batch 23, Images: 736/1000 Total_loss: 0.4859556742012501, Loss: 0.020733\n",
      "\tTraining batch 24, Images: 768/1000 Total_loss: 0.5068839136511087, Loss: 0.020928\n",
      "\tTraining batch 25, Images: 800/1000 Total_loss: 0.5293857678771019, Loss: 0.022502\n",
      "\tTraining batch 26, Images: 832/1000 Total_loss: 0.5488105267286301, Loss: 0.019425\n",
      "\tTraining batch 27, Images: 864/1000 Total_loss: 0.5693461485207081, Loss: 0.020536\n",
      "\tTraining batch 28, Images: 896/1000 Total_loss: 0.5885611530393362, Loss: 0.019215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 29, Images: 928/1000 Total_loss: 0.6114508863538504, Loss: 0.022890\n",
      "\tTraining batch 30, Images: 960/1000 Total_loss: 0.6309663262218237, Loss: 0.019515\n",
      "\tTraining batch 31, Images: 992/1000 Total_loss: 0.6522771064192057, Loss: 0.021311\n",
      "\tTraining batch 32, Images: 1024/1000 Total_loss: 0.6677866149693727, Loss: 0.015510\n",
      "Average loss for Training set 6: 0.020868\n",
      "Average loss for Validation set 6: 0.019956\n",
      "\n",
      "Epoch #  7\n",
      "\tTraining batch 1, Images: 32/1000 Total_loss: 0.016189124435186386, Loss: 0.016189\n",
      "\tTraining batch 2, Images: 64/1000 Total_loss: 0.03733770735561848, Loss: 0.021149\n",
      "\tTraining batch 3, Images: 96/1000 Total_loss: 0.05531678907573223, Loss: 0.017979\n",
      "\tTraining batch 4, Images: 128/1000 Total_loss: 0.07760589011013508, Loss: 0.022289\n",
      "\tTraining batch 5, Images: 160/1000 Total_loss: 0.09972196631133556, Loss: 0.022116\n",
      "\tTraining batch 6, Images: 192/1000 Total_loss: 0.12434652633965015, Loss: 0.024625\n",
      "\tTraining batch 7, Images: 224/1000 Total_loss: 0.14768004603683949, Loss: 0.023334\n",
      "\tTraining batch 8, Images: 256/1000 Total_loss: 0.1674252226948738, Loss: 0.019745\n",
      "\tTraining batch 9, Images: 288/1000 Total_loss: 0.18735895678400993, Loss: 0.019934\n",
      "\tTraining batch 10, Images: 320/1000 Total_loss: 0.20520702935755253, Loss: 0.017848\n",
      "\tTraining batch 11, Images: 352/1000 Total_loss: 0.2233517486602068, Loss: 0.018145\n",
      "\tTraining batch 12, Images: 384/1000 Total_loss: 0.24183309078216553, Loss: 0.018481\n",
      "\tTraining batch 13, Images: 416/1000 Total_loss: 0.2614730503410101, Loss: 0.019640\n",
      "\tTraining batch 14, Images: 448/1000 Total_loss: 0.282674053683877, Loss: 0.021201\n",
      "\tTraining batch 15, Images: 480/1000 Total_loss: 0.29916795901954174, Loss: 0.016494\n",
      "\tTraining batch 16, Images: 512/1000 Total_loss: 0.31987169198691845, Loss: 0.020704\n",
      "\tTraining batch 17, Images: 544/1000 Total_loss: 0.3449877854436636, Loss: 0.025116\n",
      "\tTraining batch 18, Images: 576/1000 Total_loss: 0.3682473264634609, Loss: 0.023260\n",
      "\tTraining batch 19, Images: 608/1000 Total_loss: 0.3883134797215462, Loss: 0.020066\n",
      "\tTraining batch 20, Images: 640/1000 Total_loss: 0.41010960191488266, Loss: 0.021796\n",
      "\tTraining batch 21, Images: 672/1000 Total_loss: 0.42759167216718197, Loss: 0.017482\n",
      "\tTraining batch 22, Images: 704/1000 Total_loss: 0.4463610965758562, Loss: 0.018769\n",
      "\tTraining batch 23, Images: 736/1000 Total_loss: 0.4652773719280958, Loss: 0.018916\n",
      "\tTraining batch 24, Images: 768/1000 Total_loss: 0.4844804275780916, Loss: 0.019203\n",
      "\tTraining batch 25, Images: 800/1000 Total_loss: 0.49767124373465776, Loss: 0.013191\n",
      "\tTraining batch 26, Images: 832/1000 Total_loss: 0.5128195090219378, Loss: 0.015148\n",
      "\tTraining batch 27, Images: 864/1000 Total_loss: 0.5318945730105042, Loss: 0.019075\n",
      "\tTraining batch 28, Images: 896/1000 Total_loss: 0.5487435841932893, Loss: 0.016849\n",
      "\tTraining batch 29, Images: 928/1000 Total_loss: 0.5723685184493661, Loss: 0.023625\n",
      "\tTraining batch 30, Images: 960/1000 Total_loss: 0.5851482031866908, Loss: 0.012780\n",
      "\tTraining batch 31, Images: 992/1000 Total_loss: 0.6043853377923369, Loss: 0.019237\n",
      "\tTraining batch 32, Images: 1024/1000 Total_loss: 0.614499562419951, Loss: 0.010114\n",
      "Average loss for Training set 7: 0.019203\n",
      "Average loss for Validation set 7: 0.016039\n",
      "\n",
      "Epoch #  8\n",
      "\tTraining batch 1, Images: 32/1000 Total_loss: 0.016457639634609222, Loss: 0.016458\n",
      "\tTraining batch 2, Images: 64/1000 Total_loss: 0.03287217393517494, Loss: 0.016415\n",
      "\tTraining batch 3, Images: 96/1000 Total_loss: 0.05075429007411003, Loss: 0.017882\n",
      "\tTraining batch 4, Images: 128/1000 Total_loss: 0.06877920962870121, Loss: 0.018025\n",
      "\tTraining batch 5, Images: 160/1000 Total_loss: 0.08549332246184349, Loss: 0.016714\n",
      "\tTraining batch 6, Images: 192/1000 Total_loss: 0.10453712940216064, Loss: 0.019044\n",
      "\tTraining batch 7, Images: 224/1000 Total_loss: 0.1214906107634306, Loss: 0.016953\n",
      "\tTraining batch 8, Images: 256/1000 Total_loss: 0.13909216970205307, Loss: 0.017602\n",
      "\tTraining batch 9, Images: 288/1000 Total_loss: 0.1549050696194172, Loss: 0.015813\n",
      "\tTraining batch 10, Images: 320/1000 Total_loss: 0.17316810600459576, Loss: 0.018263\n",
      "\tTraining batch 11, Images: 352/1000 Total_loss: 0.18837768398225307, Loss: 0.015210\n",
      "\tTraining batch 12, Images: 384/1000 Total_loss: 0.2020873362198472, Loss: 0.013710\n",
      "\tTraining batch 13, Images: 416/1000 Total_loss: 0.2204225854948163, Loss: 0.018335\n",
      "\tTraining batch 14, Images: 448/1000 Total_loss: 0.2366991387680173, Loss: 0.016277\n",
      "\tTraining batch 15, Images: 480/1000 Total_loss: 0.2519293138757348, Loss: 0.015230\n",
      "\tTraining batch 16, Images: 512/1000 Total_loss: 0.2666539289057255, Loss: 0.014725\n",
      "\tTraining batch 17, Images: 544/1000 Total_loss: 0.28155066445469856, Loss: 0.014897\n",
      "\tTraining batch 18, Images: 576/1000 Total_loss: 0.29866898432374, Loss: 0.017118\n",
      "\tTraining batch 19, Images: 608/1000 Total_loss: 0.3124697944149375, Loss: 0.013801\n",
      "\tTraining batch 20, Images: 640/1000 Total_loss: 0.32557460851967335, Loss: 0.013105\n",
      "\tTraining batch 21, Images: 672/1000 Total_loss: 0.34114960208535194, Loss: 0.015575\n",
      "\tTraining batch 22, Images: 704/1000 Total_loss: 0.35322737134993076, Loss: 0.012078\n",
      "\tTraining batch 23, Images: 736/1000 Total_loss: 0.3687762934714556, Loss: 0.015549\n",
      "\tTraining batch 24, Images: 768/1000 Total_loss: 0.38517955876886845, Loss: 0.016403\n",
      "\tTraining batch 25, Images: 800/1000 Total_loss: 0.39811364747583866, Loss: 0.012934\n",
      "\tTraining batch 26, Images: 832/1000 Total_loss: 0.41031807847321033, Loss: 0.012204\n",
      "\tTraining batch 27, Images: 864/1000 Total_loss: 0.4245907785370946, Loss: 0.014273\n",
      "\tTraining batch 28, Images: 896/1000 Total_loss: 0.4391089202836156, Loss: 0.014518\n",
      "\tTraining batch 29, Images: 928/1000 Total_loss: 0.4510068157687783, Loss: 0.011898\n",
      "\tTraining batch 30, Images: 960/1000 Total_loss: 0.46406787168234587, Loss: 0.013061\n",
      "\tTraining batch 31, Images: 992/1000 Total_loss: 0.4737272709608078, Loss: 0.009659\n",
      "\tTraining batch 32, Images: 1024/1000 Total_loss: 0.4830105993896723, Loss: 0.009283\n",
      "Average loss for Training set 8: 0.015094\n",
      "Average loss for Validation set 8: 0.015449\n",
      "\n",
      "Epoch #  9\n",
      "\tTraining batch 1, Images: 32/1000 Total_loss: 0.014027141034603119, Loss: 0.014027\n",
      "\tTraining batch 2, Images: 64/1000 Total_loss: 0.03100460395216942, Loss: 0.016977\n",
      "\tTraining batch 3, Images: 96/1000 Total_loss: 0.04526463057845831, Loss: 0.014260\n",
      "\tTraining batch 4, Images: 128/1000 Total_loss: 0.058005550876259804, Loss: 0.012741\n",
      "\tTraining batch 5, Images: 160/1000 Total_loss: 0.07159162778407335, Loss: 0.013586\n",
      "\tTraining batch 6, Images: 192/1000 Total_loss: 0.08044616878032684, Loss: 0.008855\n",
      "\tTraining batch 7, Images: 224/1000 Total_loss: 0.09299726039171219, Loss: 0.012551\n",
      "\tTraining batch 8, Images: 256/1000 Total_loss: 0.10513473208993673, Loss: 0.012137\n",
      "\tTraining batch 9, Images: 288/1000 Total_loss: 0.11744943913072348, Loss: 0.012315\n",
      "\tTraining batch 10, Images: 320/1000 Total_loss: 0.1347480220720172, Loss: 0.017299\n",
      "\tTraining batch 11, Images: 352/1000 Total_loss: 0.14905413519591093, Loss: 0.014306\n",
      "\tTraining batch 12, Images: 384/1000 Total_loss: 0.15981406159698963, Loss: 0.010760\n",
      "\tTraining batch 13, Images: 416/1000 Total_loss: 0.16961864475160837, Loss: 0.009805\n",
      "\tTraining batch 14, Images: 448/1000 Total_loss: 0.18227747734636068, Loss: 0.012659\n",
      "\tTraining batch 15, Images: 480/1000 Total_loss: 0.19656567182391882, Loss: 0.014288\n",
      "\tTraining batch 16, Images: 512/1000 Total_loss: 0.20619897451251745, Loss: 0.009633\n",
      "\tTraining batch 17, Images: 544/1000 Total_loss: 0.21987795177847147, Loss: 0.013679\n",
      "\tTraining batch 18, Images: 576/1000 Total_loss: 0.2318825824186206, Loss: 0.012005\n",
      "\tTraining batch 19, Images: 608/1000 Total_loss: 0.24594909884035587, Loss: 0.014067\n",
      "\tTraining batch 20, Images: 640/1000 Total_loss: 0.26352907344698906, Loss: 0.017580\n",
      "\tTraining batch 21, Images: 672/1000 Total_loss: 0.2728233151137829, Loss: 0.009294\n",
      "\tTraining batch 22, Images: 704/1000 Total_loss: 0.28200590517371893, Loss: 0.009183\n",
      "\tTraining batch 23, Images: 736/1000 Total_loss: 0.29635893274098635, Loss: 0.014353\n",
      "\tTraining batch 24, Images: 768/1000 Total_loss: 0.3080060901120305, Loss: 0.011647\n",
      "\tTraining batch 25, Images: 800/1000 Total_loss: 0.3194006560370326, Loss: 0.011395\n",
      "\tTraining batch 26, Images: 832/1000 Total_loss: 0.33278771582990885, Loss: 0.013387\n",
      "\tTraining batch 27, Images: 864/1000 Total_loss: 0.3468804955482483, Loss: 0.014093\n",
      "\tTraining batch 28, Images: 896/1000 Total_loss: 0.35757491551339626, Loss: 0.010694\n",
      "\tTraining batch 29, Images: 928/1000 Total_loss: 0.3679959736764431, Loss: 0.010421\n",
      "\tTraining batch 30, Images: 960/1000 Total_loss: 0.38105045910924673, Loss: 0.013054\n",
      "\tTraining batch 31, Images: 992/1000 Total_loss: 0.3914199536666274, Loss: 0.010369\n",
      "\tTraining batch 32, Images: 1024/1000 Total_loss: 0.40764300990849733, Loss: 0.016223\n",
      "Average loss for Training set 9: 0.012739\n",
      "Average loss for Validation set 9: 0.011272\n",
      "\n",
      "Epoch #  10\n",
      "\tTraining batch 1, Images: 32/1000 Total_loss: 0.011357998475432396, Loss: 0.011358\n",
      "\tTraining batch 2, Images: 64/1000 Total_loss: 0.023537793196737766, Loss: 0.012180\n",
      "\tTraining batch 3, Images: 96/1000 Total_loss: 0.03423736337572336, Loss: 0.010700\n",
      "\tTraining batch 4, Images: 128/1000 Total_loss: 0.04633605387061834, Loss: 0.012099\n",
      "\tTraining batch 5, Images: 160/1000 Total_loss: 0.05804355628788471, Loss: 0.011708\n",
      "\tTraining batch 6, Images: 192/1000 Total_loss: 0.07308372296392918, Loss: 0.015040\n",
      "\tTraining batch 7, Images: 224/1000 Total_loss: 0.08459092490375042, Loss: 0.011507\n",
      "\tTraining batch 8, Images: 256/1000 Total_loss: 0.09727813396602869, Loss: 0.012687\n",
      "\tTraining batch 9, Images: 288/1000 Total_loss: 0.11169080063700676, Loss: 0.014413\n",
      "\tTraining batch 10, Images: 320/1000 Total_loss: 0.12157202791422606, Loss: 0.009881\n",
      "\tTraining batch 11, Images: 352/1000 Total_loss: 0.1344321770593524, Loss: 0.012860\n",
      "\tTraining batch 12, Images: 384/1000 Total_loss: 0.144139158539474, Loss: 0.009707\n",
      "\tTraining batch 13, Images: 416/1000 Total_loss: 0.15343646425753832, Loss: 0.009297\n",
      "\tTraining batch 14, Images: 448/1000 Total_loss: 0.16275410633534193, Loss: 0.009318\n",
      "\tTraining batch 15, Images: 480/1000 Total_loss: 0.1733067613095045, Loss: 0.010553\n",
      "\tTraining batch 16, Images: 512/1000 Total_loss: 0.18403162248432636, Loss: 0.010725\n",
      "\tTraining batch 17, Images: 544/1000 Total_loss: 0.19679143652319908, Loss: 0.012760\n",
      "\tTraining batch 18, Images: 576/1000 Total_loss: 0.2072512973099947, Loss: 0.010460\n",
      "\tTraining batch 19, Images: 608/1000 Total_loss: 0.21749480441212654, Loss: 0.010244\n",
      "\tTraining batch 20, Images: 640/1000 Total_loss: 0.22644626162946224, Loss: 0.008951\n",
      "\tTraining batch 21, Images: 672/1000 Total_loss: 0.2328423517756164, Loss: 0.006396\n",
      "\tTraining batch 22, Images: 704/1000 Total_loss: 0.24460666393861175, Loss: 0.011764\n",
      "\tTraining batch 23, Images: 736/1000 Total_loss: 0.2572182905860245, Loss: 0.012612\n",
      "\tTraining batch 24, Images: 768/1000 Total_loss: 0.26819366170093417, Loss: 0.010975\n",
      "\tTraining batch 25, Images: 800/1000 Total_loss: 0.2772009135223925, Loss: 0.009007\n",
      "\tTraining batch 26, Images: 832/1000 Total_loss: 0.2893559797666967, Loss: 0.012155\n",
      "\tTraining batch 27, Images: 864/1000 Total_loss: 0.300952245015651, Loss: 0.011596\n",
      "\tTraining batch 28, Images: 896/1000 Total_loss: 0.31141198659315705, Loss: 0.010460\n",
      "\tTraining batch 29, Images: 928/1000 Total_loss: 0.32159282686188817, Loss: 0.010181\n",
      "\tTraining batch 30, Images: 960/1000 Total_loss: 0.3323256396688521, Loss: 0.010733\n",
      "\tTraining batch 31, Images: 992/1000 Total_loss: 0.3424571198411286, Loss: 0.010131\n",
      "\tTraining batch 32, Images: 1024/1000 Total_loss: 0.349995959084481, Loss: 0.007539\n",
      "Average loss for Training set 10: 0.010937\n",
      "Average loss for Validation set 10: 0.009592\n",
      "\n",
      "Epoch #  11\n",
      "\tTraining batch 1, Images: 32/1000 Total_loss: 0.010522304102778435, Loss: 0.010522\n",
      "\tTraining batch 2, Images: 64/1000 Total_loss: 0.0216782558709383, Loss: 0.011156\n",
      "\tTraining batch 3, Images: 96/1000 Total_loss: 0.03264188952744007, Loss: 0.010964\n",
      "\tTraining batch 4, Images: 128/1000 Total_loss: 0.042666761204600334, Loss: 0.010025\n",
      "\tTraining batch 5, Images: 160/1000 Total_loss: 0.05328957084566355, Loss: 0.010623\n",
      "\tTraining batch 6, Images: 192/1000 Total_loss: 0.06376774609088898, Loss: 0.010478\n",
      "\tTraining batch 7, Images: 224/1000 Total_loss: 0.07177825178951025, Loss: 0.008011\n",
      "\tTraining batch 8, Images: 256/1000 Total_loss: 0.08031261991709471, Loss: 0.008534\n",
      "\tTraining batch 9, Images: 288/1000 Total_loss: 0.08940586261451244, Loss: 0.009093\n",
      "\tTraining batch 10, Images: 320/1000 Total_loss: 0.09943021275103092, Loss: 0.010024\n",
      "\tTraining batch 11, Images: 352/1000 Total_loss: 0.11130000464618206, Loss: 0.011870\n",
      "\tTraining batch 12, Images: 384/1000 Total_loss: 0.12166690174490213, Loss: 0.010367\n",
      "\tTraining batch 13, Images: 416/1000 Total_loss: 0.13424161169677973, Loss: 0.012575\n",
      "\tTraining batch 14, Images: 448/1000 Total_loss: 0.14179900847375393, Loss: 0.007557\n",
      "\tTraining batch 15, Images: 480/1000 Total_loss: 0.14971996564418077, Loss: 0.007921\n",
      "\tTraining batch 16, Images: 512/1000 Total_loss: 0.15951642487198114, Loss: 0.009796\n",
      "\tTraining batch 17, Images: 544/1000 Total_loss: 0.16920291632413864, Loss: 0.009686\n",
      "\tTraining batch 18, Images: 576/1000 Total_loss: 0.18054077681154013, Loss: 0.011338\n",
      "\tTraining batch 19, Images: 608/1000 Total_loss: 0.18761236499994993, Loss: 0.007072\n",
      "\tTraining batch 20, Images: 640/1000 Total_loss: 0.195416992995888, Loss: 0.007805\n",
      "\tTraining batch 21, Images: 672/1000 Total_loss: 0.20730902394279838, Loss: 0.011892\n",
      "\tTraining batch 22, Images: 704/1000 Total_loss: 0.2165348711423576, Loss: 0.009226\n",
      "\tTraining batch 23, Images: 736/1000 Total_loss: 0.22681402368471026, Loss: 0.010279\n",
      "\tTraining batch 24, Images: 768/1000 Total_loss: 0.2338797808624804, Loss: 0.007066\n",
      "\tTraining batch 25, Images: 800/1000 Total_loss: 0.24437875347211957, Loss: 0.010499\n",
      "\tTraining batch 26, Images: 832/1000 Total_loss: 0.25379512133076787, Loss: 0.009416\n",
      "\tTraining batch 27, Images: 864/1000 Total_loss: 0.2643172857351601, Loss: 0.010522\n",
      "\tTraining batch 28, Images: 896/1000 Total_loss: 0.2755106561817229, Loss: 0.011193\n",
      "\tTraining batch 29, Images: 928/1000 Total_loss: 0.2846340951509774, Loss: 0.009123\n",
      "\tTraining batch 30, Images: 960/1000 Total_loss: 0.2922955472022295, Loss: 0.007661\n",
      "\tTraining batch 31, Images: 992/1000 Total_loss: 0.30281026754528284, Loss: 0.010515\n",
      "\tTraining batch 32, Images: 1024/1000 Total_loss: 0.30847652070224285, Loss: 0.005666\n",
      "Average loss for Training set 11: 0.009640\n",
      "Average loss for Validation set 11: 0.007913\n",
      "\n",
      "Epoch #  12\n",
      "\tTraining batch 1, Images: 32/1000 Total_loss: 0.0064574903808534145, Loss: 0.006457\n",
      "\tTraining batch 2, Images: 64/1000 Total_loss: 0.01580946473404765, Loss: 0.009352\n",
      "\tTraining batch 3, Images: 96/1000 Total_loss: 0.02648221282288432, Loss: 0.010673\n",
      "\tTraining batch 4, Images: 128/1000 Total_loss: 0.032758493442088366, Loss: 0.006276\n",
      "\tTraining batch 5, Images: 160/1000 Total_loss: 0.042394562158733606, Loss: 0.009636\n",
      "\tTraining batch 6, Images: 192/1000 Total_loss: 0.052683521527796984, Loss: 0.010289\n",
      "\tTraining batch 7, Images: 224/1000 Total_loss: 0.06147545436397195, Loss: 0.008792\n",
      "\tTraining batch 8, Images: 256/1000 Total_loss: 0.06964722694829106, Loss: 0.008172\n",
      "\tTraining batch 9, Images: 288/1000 Total_loss: 0.07911043940111995, Loss: 0.009463\n",
      "\tTraining batch 10, Images: 320/1000 Total_loss: 0.08832293981686234, Loss: 0.009213\n",
      "\tTraining batch 11, Images: 352/1000 Total_loss: 0.09568433230742812, Loss: 0.007361\n",
      "\tTraining batch 12, Images: 384/1000 Total_loss: 0.10647179232910275, Loss: 0.010787\n",
      "\tTraining batch 13, Images: 416/1000 Total_loss: 0.11499364534392953, Loss: 0.008522\n",
      "\tTraining batch 14, Images: 448/1000 Total_loss: 0.12578377267345786, Loss: 0.010790\n",
      "\tTraining batch 15, Images: 480/1000 Total_loss: 0.13111983332782984, Loss: 0.005336\n",
      "\tTraining batch 16, Images: 512/1000 Total_loss: 0.13974725920706987, Loss: 0.008627\n",
      "\tTraining batch 17, Images: 544/1000 Total_loss: 0.1471208594739437, Loss: 0.007374\n",
      "\tTraining batch 18, Images: 576/1000 Total_loss: 0.1583267217501998, Loss: 0.011206\n",
      "\tTraining batch 19, Images: 608/1000 Total_loss: 0.16705482359975576, Loss: 0.008728\n",
      "\tTraining batch 20, Images: 640/1000 Total_loss: 0.17458133213222027, Loss: 0.007527\n",
      "\tTraining batch 21, Images: 672/1000 Total_loss: 0.18369463179260492, Loss: 0.009113\n",
      "\tTraining batch 22, Images: 704/1000 Total_loss: 0.1925951922312379, Loss: 0.008901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 23, Images: 736/1000 Total_loss: 0.20230083912611008, Loss: 0.009706\n",
      "\tTraining batch 24, Images: 768/1000 Total_loss: 0.2082164823077619, Loss: 0.005916\n",
      "\tTraining batch 25, Images: 800/1000 Total_loss: 0.21796782268211246, Loss: 0.009751\n",
      "\tTraining batch 26, Images: 832/1000 Total_loss: 0.2274006656371057, Loss: 0.009433\n",
      "\tTraining batch 27, Images: 864/1000 Total_loss: 0.23387978365644813, Loss: 0.006479\n",
      "\tTraining batch 28, Images: 896/1000 Total_loss: 0.2427820940501988, Loss: 0.008902\n",
      "\tTraining batch 29, Images: 928/1000 Total_loss: 0.25151212560012937, Loss: 0.008730\n",
      "\tTraining batch 30, Images: 960/1000 Total_loss: 0.26041090162470937, Loss: 0.008899\n",
      "\tTraining batch 31, Images: 992/1000 Total_loss: 0.26864172192290425, Loss: 0.008231\n",
      "\tTraining batch 32, Images: 1024/1000 Total_loss: 0.27775593707337976, Loss: 0.009114\n",
      "Average loss for Training set 12: 0.008680\n",
      "Average loss for Validation set 12: 0.007252\n",
      "\n",
      "Epoch #  13\n",
      "\tTraining batch 1, Images: 32/1000 Total_loss: 0.00748058408498764, Loss: 0.007481\n",
      "\tTraining batch 2, Images: 64/1000 Total_loss: 0.01554361917078495, Loss: 0.008063\n",
      "\tTraining batch 3, Images: 96/1000 Total_loss: 0.02333176089450717, Loss: 0.007788\n",
      "\tTraining batch 4, Images: 128/1000 Total_loss: 0.03227995755150914, Loss: 0.008948\n",
      "\tTraining batch 5, Images: 160/1000 Total_loss: 0.039748622104525566, Loss: 0.007469\n",
      "\tTraining batch 6, Images: 192/1000 Total_loss: 0.045626734383404255, Loss: 0.005878\n",
      "\tTraining batch 7, Images: 224/1000 Total_loss: 0.0538144176825881, Loss: 0.008188\n",
      "\tTraining batch 8, Images: 256/1000 Total_loss: 0.0636358018964529, Loss: 0.009821\n",
      "\tTraining batch 9, Images: 288/1000 Total_loss: 0.07294672727584839, Loss: 0.009311\n",
      "\tTraining batch 10, Images: 320/1000 Total_loss: 0.08180711045861244, Loss: 0.008860\n",
      "\tTraining batch 11, Images: 352/1000 Total_loss: 0.08941866038367152, Loss: 0.007612\n",
      "\tTraining batch 12, Images: 384/1000 Total_loss: 0.09673860389739275, Loss: 0.007320\n",
      "\tTraining batch 13, Images: 416/1000 Total_loss: 0.10426169913262129, Loss: 0.007523\n",
      "\tTraining batch 14, Images: 448/1000 Total_loss: 0.11107136122882366, Loss: 0.006810\n",
      "\tTraining batch 15, Images: 480/1000 Total_loss: 0.11993198655545712, Loss: 0.008861\n",
      "\tTraining batch 16, Images: 512/1000 Total_loss: 0.12662841705605388, Loss: 0.006696\n",
      "\tTraining batch 17, Images: 544/1000 Total_loss: 0.1347658564336598, Loss: 0.008137\n",
      "\tTraining batch 18, Images: 576/1000 Total_loss: 0.14209015341475606, Loss: 0.007324\n",
      "\tTraining batch 19, Images: 608/1000 Total_loss: 0.1506837080232799, Loss: 0.008594\n",
      "\tTraining batch 20, Images: 640/1000 Total_loss: 0.15888717351481318, Loss: 0.008203\n",
      "\tTraining batch 21, Images: 672/1000 Total_loss: 0.17048081709071994, Loss: 0.011594\n",
      "\tTraining batch 22, Images: 704/1000 Total_loss: 0.17797968024387956, Loss: 0.007499\n",
      "\tTraining batch 23, Images: 736/1000 Total_loss: 0.18501866841688752, Loss: 0.007039\n",
      "\tTraining batch 24, Images: 768/1000 Total_loss: 0.19013001257553697, Loss: 0.005111\n",
      "\tTraining batch 25, Images: 800/1000 Total_loss: 0.19747858121991158, Loss: 0.007349\n",
      "\tTraining batch 26, Images: 832/1000 Total_loss: 0.20588555186986923, Loss: 0.008407\n",
      "\tTraining batch 27, Images: 864/1000 Total_loss: 0.21376549173146486, Loss: 0.007880\n",
      "\tTraining batch 28, Images: 896/1000 Total_loss: 0.2207532487809658, Loss: 0.006988\n",
      "\tTraining batch 29, Images: 928/1000 Total_loss: 0.22885333187878132, Loss: 0.008100\n",
      "\tTraining batch 30, Images: 960/1000 Total_loss: 0.23456910019740462, Loss: 0.005716\n",
      "\tTraining batch 31, Images: 992/1000 Total_loss: 0.24002743046730757, Loss: 0.005458\n",
      "\tTraining batch 32, Images: 1024/1000 Total_loss: 0.24635176919400692, Loss: 0.006324\n",
      "Average loss for Training set 13: 0.007698\n",
      "Average loss for Validation set 13: 0.006377\n",
      "\n",
      "Epoch #  14\n",
      "\tTraining batch 1, Images: 32/1000 Total_loss: 0.005094071850180626, Loss: 0.005094\n",
      "\tTraining batch 2, Images: 64/1000 Total_loss: 0.009983045049011707, Loss: 0.004889\n",
      "\tTraining batch 3, Images: 96/1000 Total_loss: 0.01871148869395256, Loss: 0.008728\n",
      "\tTraining batch 4, Images: 128/1000 Total_loss: 0.025053731631487608, Loss: 0.006342\n",
      "\tTraining batch 5, Images: 160/1000 Total_loss: 0.03085365379229188, Loss: 0.005800\n",
      "\tTraining batch 6, Images: 192/1000 Total_loss: 0.03597167832776904, Loss: 0.005118\n",
      "\tTraining batch 7, Images: 224/1000 Total_loss: 0.04180248314514756, Loss: 0.005831\n",
      "\tTraining batch 8, Images: 256/1000 Total_loss: 0.048323160503059626, Loss: 0.006521\n",
      "\tTraining batch 9, Images: 288/1000 Total_loss: 0.05561467492952943, Loss: 0.007292\n",
      "\tTraining batch 10, Images: 320/1000 Total_loss: 0.0625093593262136, Loss: 0.006895\n",
      "\tTraining batch 11, Images: 352/1000 Total_loss: 0.07204427337273955, Loss: 0.009535\n",
      "\tTraining batch 12, Images: 384/1000 Total_loss: 0.08070715283975005, Loss: 0.008663\n",
      "\tTraining batch 13, Images: 416/1000 Total_loss: 0.08542099129408598, Loss: 0.004714\n",
      "\tTraining batch 14, Images: 448/1000 Total_loss: 0.09102354617789388, Loss: 0.005603\n",
      "\tTraining batch 15, Images: 480/1000 Total_loss: 0.0981267592869699, Loss: 0.007103\n",
      "\tTraining batch 16, Images: 512/1000 Total_loss: 0.10296878684312105, Loss: 0.004842\n",
      "\tTraining batch 17, Images: 544/1000 Total_loss: 0.10964122042059898, Loss: 0.006672\n",
      "\tTraining batch 18, Images: 576/1000 Total_loss: 0.11530446354299784, Loss: 0.005663\n",
      "\tTraining batch 19, Images: 608/1000 Total_loss: 0.12262587621808052, Loss: 0.007321\n",
      "\tTraining batch 20, Images: 640/1000 Total_loss: 0.12894047237932682, Loss: 0.006315\n",
      "\tTraining batch 21, Images: 672/1000 Total_loss: 0.13430938683450222, Loss: 0.005369\n",
      "\tTraining batch 22, Images: 704/1000 Total_loss: 0.14306641090661287, Loss: 0.008757\n",
      "\tTraining batch 23, Images: 736/1000 Total_loss: 0.1490022875368595, Loss: 0.005936\n",
      "\tTraining batch 24, Images: 768/1000 Total_loss: 0.1553400820121169, Loss: 0.006338\n",
      "\tTraining batch 25, Images: 800/1000 Total_loss: 0.16210469510406256, Loss: 0.006765\n",
      "\tTraining batch 26, Images: 832/1000 Total_loss: 0.16637256788089871, Loss: 0.004268\n",
      "\tTraining batch 27, Images: 864/1000 Total_loss: 0.17344279354438186, Loss: 0.007070\n",
      "\tTraining batch 28, Images: 896/1000 Total_loss: 0.18062208127230406, Loss: 0.007179\n",
      "\tTraining batch 29, Images: 928/1000 Total_loss: 0.18627132195979357, Loss: 0.005649\n",
      "\tTraining batch 30, Images: 960/1000 Total_loss: 0.1922635049559176, Loss: 0.005992\n",
      "\tTraining batch 31, Images: 992/1000 Total_loss: 0.19807581743225455, Loss: 0.005812\n",
      "\tTraining batch 32, Images: 1024/1000 Total_loss: 0.20356502709910274, Loss: 0.005489\n",
      "Average loss for Training set 14: 0.006361\n",
      "Average loss for Validation set 14: 0.005614\n",
      "\n",
      "Epoch #  15\n",
      "\tTraining batch 1, Images: 32/1000 Total_loss: 0.007583150174468756, Loss: 0.007583\n",
      "\tTraining batch 2, Images: 64/1000 Total_loss: 0.011788212228566408, Loss: 0.004205\n",
      "\tTraining batch 3, Images: 96/1000 Total_loss: 0.016550205647945404, Loss: 0.004762\n",
      "\tTraining batch 4, Images: 128/1000 Total_loss: 0.024796487763524055, Loss: 0.008246\n",
      "\tTraining batch 5, Images: 160/1000 Total_loss: 0.03103789873421192, Loss: 0.006241\n",
      "\tTraining batch 6, Images: 192/1000 Total_loss: 0.0373003538697958, Loss: 0.006262\n",
      "\tTraining batch 7, Images: 224/1000 Total_loss: 0.042653807904571295, Loss: 0.005353\n",
      "\tTraining batch 8, Images: 256/1000 Total_loss: 0.04709360934793949, Loss: 0.004440\n",
      "\tTraining batch 9, Images: 288/1000 Total_loss: 0.05164611432701349, Loss: 0.004553\n",
      "\tTraining batch 10, Images: 320/1000 Total_loss: 0.057522861286997795, Loss: 0.005877\n",
      "\tTraining batch 11, Images: 352/1000 Total_loss: 0.06236561434343457, Loss: 0.004843\n",
      "\tTraining batch 12, Images: 384/1000 Total_loss: 0.06837710132822394, Loss: 0.006011\n",
      "\tTraining batch 13, Images: 416/1000 Total_loss: 0.07621570443734527, Loss: 0.007839\n",
      "\tTraining batch 14, Images: 448/1000 Total_loss: 0.08195434603840113, Loss: 0.005739\n",
      "\tTraining batch 15, Images: 480/1000 Total_loss: 0.08789948467165232, Loss: 0.005945\n",
      "\tTraining batch 16, Images: 512/1000 Total_loss: 0.09529028134420514, Loss: 0.007391\n",
      "\tTraining batch 17, Images: 544/1000 Total_loss: 0.10028939647600055, Loss: 0.004999\n",
      "\tTraining batch 18, Images: 576/1000 Total_loss: 0.10725121898576617, Loss: 0.006962\n",
      "\tTraining batch 19, Images: 608/1000 Total_loss: 0.11116409813985229, Loss: 0.003913\n",
      "\tTraining batch 20, Images: 640/1000 Total_loss: 0.11484258202835917, Loss: 0.003678\n",
      "\tTraining batch 21, Images: 672/1000 Total_loss: 0.1215852671302855, Loss: 0.006743\n",
      "\tTraining batch 22, Images: 704/1000 Total_loss: 0.1267426130361855, Loss: 0.005157\n",
      "\tTraining batch 23, Images: 736/1000 Total_loss: 0.13252818072214723, Loss: 0.005786\n",
      "\tTraining batch 24, Images: 768/1000 Total_loss: 0.1380312773399055, Loss: 0.005503\n",
      "\tTraining batch 25, Images: 800/1000 Total_loss: 0.14535910915583372, Loss: 0.007328\n",
      "\tTraining batch 26, Images: 832/1000 Total_loss: 0.15137765230610967, Loss: 0.006019\n",
      "\tTraining batch 27, Images: 864/1000 Total_loss: 0.15631235064938664, Loss: 0.004935\n",
      "\tTraining batch 28, Images: 896/1000 Total_loss: 0.16192799480631948, Loss: 0.005616\n",
      "\tTraining batch 29, Images: 928/1000 Total_loss: 0.1679731085896492, Loss: 0.006045\n",
      "\tTraining batch 30, Images: 960/1000 Total_loss: 0.1757016433402896, Loss: 0.007729\n",
      "\tTraining batch 31, Images: 992/1000 Total_loss: 0.18182800710201263, Loss: 0.006126\n",
      "\tTraining batch 32, Images: 1024/1000 Total_loss: 0.186866064555943, Loss: 0.005038\n",
      "Average loss for Training set 15: 0.005840\n",
      "Average loss for Validation set 15: 0.004373\n",
      "\n",
      "Epoch #  16\n",
      "\tTraining batch 1, Images: 32/1000 Total_loss: 0.004542497918009758, Loss: 0.004542\n",
      "\tTraining batch 2, Images: 64/1000 Total_loss: 0.009739416651427746, Loss: 0.005197\n",
      "\tTraining batch 3, Images: 96/1000 Total_loss: 0.015011320821940899, Loss: 0.005272\n",
      "\tTraining batch 4, Images: 128/1000 Total_loss: 0.02022087387740612, Loss: 0.005210\n",
      "\tTraining batch 5, Images: 160/1000 Total_loss: 0.027742384001612663, Loss: 0.007522\n",
      "\tTraining batch 6, Images: 192/1000 Total_loss: 0.03380161244422197, Loss: 0.006059\n",
      "\tTraining batch 7, Images: 224/1000 Total_loss: 0.03968122508376837, Loss: 0.005880\n",
      "\tTraining batch 8, Images: 256/1000 Total_loss: 0.046100763604044914, Loss: 0.006420\n",
      "\tTraining batch 9, Images: 288/1000 Total_loss: 0.0510308975353837, Loss: 0.004930\n",
      "\tTraining batch 10, Images: 320/1000 Total_loss: 0.05634276056662202, Loss: 0.005312\n",
      "\tTraining batch 11, Images: 352/1000 Total_loss: 0.06132729956880212, Loss: 0.004985\n",
      "\tTraining batch 12, Images: 384/1000 Total_loss: 0.06470739236101508, Loss: 0.003380\n",
      "\tTraining batch 13, Images: 416/1000 Total_loss: 0.06917952327057719, Loss: 0.004472\n",
      "\tTraining batch 14, Images: 448/1000 Total_loss: 0.07300962414592505, Loss: 0.003830\n",
      "\tTraining batch 15, Images: 480/1000 Total_loss: 0.07954599894583225, Loss: 0.006536\n",
      "\tTraining batch 16, Images: 512/1000 Total_loss: 0.08770550321787596, Loss: 0.008160\n",
      "\tTraining batch 17, Images: 544/1000 Total_loss: 0.09485803358256817, Loss: 0.007153\n",
      "\tTraining batch 18, Images: 576/1000 Total_loss: 0.10031552752479911, Loss: 0.005457\n",
      "\tTraining batch 19, Images: 608/1000 Total_loss: 0.10529077518731356, Loss: 0.004975\n",
      "\tTraining batch 20, Images: 640/1000 Total_loss: 0.11054207105189562, Loss: 0.005251\n",
      "\tTraining batch 21, Images: 672/1000 Total_loss: 0.11504385992884636, Loss: 0.004502\n",
      "\tTraining batch 22, Images: 704/1000 Total_loss: 0.12160230008885264, Loss: 0.006558\n",
      "\tTraining batch 23, Images: 736/1000 Total_loss: 0.12658080691471696, Loss: 0.004979\n",
      "\tTraining batch 24, Images: 768/1000 Total_loss: 0.13303524116054177, Loss: 0.006454\n",
      "\tTraining batch 25, Images: 800/1000 Total_loss: 0.13645378546789289, Loss: 0.003419\n",
      "\tTraining batch 26, Images: 832/1000 Total_loss: 0.14023331552743912, Loss: 0.003780\n",
      "\tTraining batch 27, Images: 864/1000 Total_loss: 0.14586818125098944, Loss: 0.005635\n",
      "\tTraining batch 28, Images: 896/1000 Total_loss: 0.1507851192727685, Loss: 0.004917\n",
      "\tTraining batch 29, Images: 928/1000 Total_loss: 0.15634883055463433, Loss: 0.005564\n",
      "\tTraining batch 30, Images: 960/1000 Total_loss: 0.1612154827453196, Loss: 0.004867\n",
      "\tTraining batch 31, Images: 992/1000 Total_loss: 0.16556787258014083, Loss: 0.004352\n",
      "\tTraining batch 32, Images: 1024/1000 Total_loss: 0.17417991673573852, Loss: 0.008612\n",
      "Average loss for Training set 16: 0.005443\n",
      "Average loss for Validation set 16: 0.003860\n",
      "\n",
      "Epoch #  17\n",
      "\tTraining batch 1, Images: 32/1000 Total_loss: 0.0045679472386837006, Loss: 0.004568\n",
      "\tTraining batch 2, Images: 64/1000 Total_loss: 0.010199574753642082, Loss: 0.005632\n",
      "\tTraining batch 3, Images: 96/1000 Total_loss: 0.014237860217690468, Loss: 0.004038\n",
      "\tTraining batch 4, Images: 128/1000 Total_loss: 0.01927772630006075, Loss: 0.005040\n",
      "\tTraining batch 5, Images: 160/1000 Total_loss: 0.024379652924835682, Loss: 0.005102\n",
      "\tTraining batch 6, Images: 192/1000 Total_loss: 0.02792444615624845, Loss: 0.003545\n",
      "\tTraining batch 7, Images: 224/1000 Total_loss: 0.030611581867560744, Loss: 0.002687\n",
      "\tTraining batch 8, Images: 256/1000 Total_loss: 0.034343861509114504, Loss: 0.003732\n",
      "\tTraining batch 9, Images: 288/1000 Total_loss: 0.0399534679017961, Loss: 0.005610\n",
      "\tTraining batch 10, Images: 320/1000 Total_loss: 0.04681431036442518, Loss: 0.006861\n",
      "\tTraining batch 11, Images: 352/1000 Total_loss: 0.05329278018325567, Loss: 0.006478\n",
      "\tTraining batch 12, Images: 384/1000 Total_loss: 0.05749903665855527, Loss: 0.004206\n",
      "\tTraining batch 13, Images: 416/1000 Total_loss: 0.062003565952181816, Loss: 0.004505\n",
      "\tTraining batch 14, Images: 448/1000 Total_loss: 0.06688595656305552, Loss: 0.004882\n",
      "\tTraining batch 15, Images: 480/1000 Total_loss: 0.07115459721535444, Loss: 0.004269\n",
      "\tTraining batch 16, Images: 512/1000 Total_loss: 0.0753633277490735, Loss: 0.004209\n",
      "\tTraining batch 17, Images: 544/1000 Total_loss: 0.08023936208337545, Loss: 0.004876\n",
      "\tTraining batch 18, Images: 576/1000 Total_loss: 0.0862009059637785, Loss: 0.005962\n",
      "\tTraining batch 19, Images: 608/1000 Total_loss: 0.09068368561565876, Loss: 0.004483\n",
      "\tTraining batch 20, Images: 640/1000 Total_loss: 0.09412723034620285, Loss: 0.003444\n",
      "\tTraining batch 21, Images: 672/1000 Total_loss: 0.09838367812335491, Loss: 0.004256\n",
      "\tTraining batch 22, Images: 704/1000 Total_loss: 0.1028057299554348, Loss: 0.004422\n",
      "\tTraining batch 23, Images: 736/1000 Total_loss: 0.1065589792560786, Loss: 0.003753\n",
      "\tTraining batch 24, Images: 768/1000 Total_loss: 0.11015597591176629, Loss: 0.003597\n",
      "\tTraining batch 25, Images: 800/1000 Total_loss: 0.11451564449816942, Loss: 0.004360\n",
      "\tTraining batch 26, Images: 832/1000 Total_loss: 0.11897167563438416, Loss: 0.004456\n",
      "\tTraining batch 27, Images: 864/1000 Total_loss: 0.12379560433328152, Loss: 0.004824\n",
      "\tTraining batch 28, Images: 896/1000 Total_loss: 0.12815983314067125, Loss: 0.004364\n",
      "\tTraining batch 29, Images: 928/1000 Total_loss: 0.13223498687148094, Loss: 0.004075\n",
      "\tTraining batch 30, Images: 960/1000 Total_loss: 0.1366641349159181, Loss: 0.004429\n",
      "\tTraining batch 31, Images: 992/1000 Total_loss: 0.14150400878861547, Loss: 0.004840\n",
      "\tTraining batch 32, Images: 1024/1000 Total_loss: 0.14461925625801086, Loss: 0.003115\n",
      "Average loss for Training set 17: 0.004519\n",
      "Average loss for Validation set 17: 0.003180\n",
      "\n",
      "Epoch #  18\n",
      "\tTraining batch 1, Images: 32/1000 Total_loss: 0.003051513573154807, Loss: 0.003052\n",
      "\tTraining batch 2, Images: 64/1000 Total_loss: 0.007238103309646249, Loss: 0.004187\n",
      "\tTraining batch 3, Images: 96/1000 Total_loss: 0.013064635219052434, Loss: 0.005827\n",
      "\tTraining batch 4, Images: 128/1000 Total_loss: 0.016038453206419945, Loss: 0.002974\n",
      "\tTraining batch 5, Images: 160/1000 Total_loss: 0.021477022673934698, Loss: 0.005439\n",
      "\tTraining batch 6, Images: 192/1000 Total_loss: 0.02500599017366767, Loss: 0.003529\n",
      "\tTraining batch 7, Images: 224/1000 Total_loss: 0.028815842466428876, Loss: 0.003810\n",
      "\tTraining batch 8, Images: 256/1000 Total_loss: 0.034203357296064496, Loss: 0.005388\n",
      "\tTraining batch 9, Images: 288/1000 Total_loss: 0.03919555549509823, Loss: 0.004992\n",
      "\tTraining batch 10, Images: 320/1000 Total_loss: 0.04238203843124211, Loss: 0.003186\n",
      "\tTraining batch 11, Images: 352/1000 Total_loss: 0.04529041307978332, Loss: 0.002908\n",
      "\tTraining batch 12, Images: 384/1000 Total_loss: 0.048537393566221, Loss: 0.003247\n",
      "\tTraining batch 13, Images: 416/1000 Total_loss: 0.051743737421929836, Loss: 0.003206\n",
      "\tTraining batch 14, Images: 448/1000 Total_loss: 0.055649138521403074, Loss: 0.003905\n",
      "\tTraining batch 15, Images: 480/1000 Total_loss: 0.05950920423492789, Loss: 0.003860\n",
      "\tTraining batch 16, Images: 512/1000 Total_loss: 0.06354081677272916, Loss: 0.004032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 17, Images: 544/1000 Total_loss: 0.06624690559692681, Loss: 0.002706\n",
      "\tTraining batch 18, Images: 576/1000 Total_loss: 0.07085129641927779, Loss: 0.004604\n",
      "\tTraining batch 19, Images: 608/1000 Total_loss: 0.07477398007176816, Loss: 0.003923\n",
      "\tTraining batch 20, Images: 640/1000 Total_loss: 0.07818740280345082, Loss: 0.003413\n",
      "\tTraining batch 21, Images: 672/1000 Total_loss: 0.08102274313569069, Loss: 0.002835\n",
      "\tTraining batch 22, Images: 704/1000 Total_loss: 0.0836071704979986, Loss: 0.002584\n",
      "\tTraining batch 23, Images: 736/1000 Total_loss: 0.08823954849503934, Loss: 0.004632\n",
      "\tTraining batch 24, Images: 768/1000 Total_loss: 0.09249612432904541, Loss: 0.004257\n",
      "\tTraining batch 25, Images: 800/1000 Total_loss: 0.09611106407828629, Loss: 0.003615\n",
      "\tTraining batch 26, Images: 832/1000 Total_loss: 0.09960851073265076, Loss: 0.003497\n",
      "\tTraining batch 27, Images: 864/1000 Total_loss: 0.10282719926908612, Loss: 0.003219\n",
      "\tTraining batch 28, Images: 896/1000 Total_loss: 0.1067553092725575, Loss: 0.003928\n",
      "\tTraining batch 29, Images: 928/1000 Total_loss: 0.11085752584040165, Loss: 0.004102\n",
      "\tTraining batch 30, Images: 960/1000 Total_loss: 0.11397546529769897, Loss: 0.003118\n",
      "\tTraining batch 31, Images: 992/1000 Total_loss: 0.11654586810618639, Loss: 0.002570\n",
      "\tTraining batch 32, Images: 1024/1000 Total_loss: 0.11913343891501427, Loss: 0.002588\n",
      "Average loss for Training set 18: 0.003723\n",
      "Average loss for Validation set 18: 0.003287\n",
      "\n",
      "Epoch #  19\n",
      "\tTraining batch 1, Images: 32/1000 Total_loss: 0.003006645245477557, Loss: 0.003007\n",
      "\tTraining batch 2, Images: 64/1000 Total_loss: 0.007196867605671287, Loss: 0.004190\n",
      "\tTraining batch 3, Images: 96/1000 Total_loss: 0.011391387553885579, Loss: 0.004195\n",
      "\tTraining batch 4, Images: 128/1000 Total_loss: 0.014442616375163198, Loss: 0.003051\n",
      "\tTraining batch 5, Images: 160/1000 Total_loss: 0.0191684786695987, Loss: 0.004726\n",
      "\tTraining batch 6, Images: 192/1000 Total_loss: 0.022926498902961612, Loss: 0.003758\n",
      "\tTraining batch 7, Images: 224/1000 Total_loss: 0.02646066597662866, Loss: 0.003534\n",
      "\tTraining batch 8, Images: 256/1000 Total_loss: 0.030550823779776692, Loss: 0.004090\n",
      "\tTraining batch 9, Images: 288/1000 Total_loss: 0.03295542276464403, Loss: 0.002405\n",
      "\tTraining batch 10, Images: 320/1000 Total_loss: 0.035835495917126536, Loss: 0.002880\n",
      "\tTraining batch 11, Images: 352/1000 Total_loss: 0.03853231971152127, Loss: 0.002697\n",
      "\tTraining batch 12, Images: 384/1000 Total_loss: 0.042606674833223224, Loss: 0.004074\n",
      "\tTraining batch 13, Images: 416/1000 Total_loss: 0.04578552907332778, Loss: 0.003179\n",
      "\tTraining batch 14, Images: 448/1000 Total_loss: 0.04975066939368844, Loss: 0.003965\n",
      "\tTraining batch 15, Images: 480/1000 Total_loss: 0.05257116933353245, Loss: 0.002820\n",
      "\tTraining batch 16, Images: 512/1000 Total_loss: 0.056111780228093266, Loss: 0.003541\n",
      "\tTraining batch 17, Images: 544/1000 Total_loss: 0.05995991569943726, Loss: 0.003848\n",
      "\tTraining batch 18, Images: 576/1000 Total_loss: 0.06322676525451243, Loss: 0.003267\n",
      "\tTraining batch 19, Images: 608/1000 Total_loss: 0.0670601932797581, Loss: 0.003833\n",
      "\tTraining batch 20, Images: 640/1000 Total_loss: 0.0699298307299614, Loss: 0.002870\n",
      "\tTraining batch 21, Images: 672/1000 Total_loss: 0.07343350420705974, Loss: 0.003504\n",
      "\tTraining batch 22, Images: 704/1000 Total_loss: 0.07593724480830133, Loss: 0.002504\n",
      "\tTraining batch 23, Images: 736/1000 Total_loss: 0.08048290456645191, Loss: 0.004546\n",
      "\tTraining batch 24, Images: 768/1000 Total_loss: 0.0840165198314935, Loss: 0.003534\n",
      "\tTraining batch 25, Images: 800/1000 Total_loss: 0.08717970736324787, Loss: 0.003163\n",
      "\tTraining batch 26, Images: 832/1000 Total_loss: 0.09103184542618692, Loss: 0.003852\n",
      "\tTraining batch 27, Images: 864/1000 Total_loss: 0.09492523455992341, Loss: 0.003893\n",
      "\tTraining batch 28, Images: 896/1000 Total_loss: 0.09995428519323468, Loss: 0.005029\n",
      "\tTraining batch 29, Images: 928/1000 Total_loss: 0.10337120108306408, Loss: 0.003417\n",
      "\tTraining batch 30, Images: 960/1000 Total_loss: 0.10760801937431097, Loss: 0.004237\n",
      "\tTraining batch 31, Images: 992/1000 Total_loss: 0.11112277861684561, Loss: 0.003515\n",
      "\tTraining batch 32, Images: 1024/1000 Total_loss: 0.1145763173699379, Loss: 0.003454\n",
      "Average loss for Training set 19: 0.003581\n",
      "Average loss for Validation set 19: 0.002396\n",
      "\n",
      "Epoch #  20\n",
      "\tTraining batch 1, Images: 32/1000 Total_loss: 0.003964891191571951, Loss: 0.003965\n",
      "\tTraining batch 2, Images: 64/1000 Total_loss: 0.007460302906110883, Loss: 0.003495\n",
      "\tTraining batch 3, Images: 96/1000 Total_loss: 0.010332469129934907, Loss: 0.002872\n",
      "\tTraining batch 4, Images: 128/1000 Total_loss: 0.014397310325875878, Loss: 0.004065\n",
      "\tTraining batch 5, Images: 160/1000 Total_loss: 0.01876747445203364, Loss: 0.004370\n",
      "\tTraining batch 6, Images: 192/1000 Total_loss: 0.02125119441188872, Loss: 0.002484\n",
      "\tTraining batch 7, Images: 224/1000 Total_loss: 0.024744679452851415, Loss: 0.003493\n",
      "\tTraining batch 8, Images: 256/1000 Total_loss: 0.02808170090429485, Loss: 0.003337\n",
      "\tTraining batch 9, Images: 288/1000 Total_loss: 0.032039726385846734, Loss: 0.003958\n",
      "\tTraining batch 10, Images: 320/1000 Total_loss: 0.034863715060055256, Loss: 0.002824\n",
      "\tTraining batch 11, Images: 352/1000 Total_loss: 0.03896539472043514, Loss: 0.004102\n",
      "\tTraining batch 12, Images: 384/1000 Total_loss: 0.041776059893891215, Loss: 0.002811\n",
      "\tTraining batch 13, Images: 416/1000 Total_loss: 0.045290634501725435, Loss: 0.003515\n",
      "\tTraining batch 14, Images: 448/1000 Total_loss: 0.048377223778516054, Loss: 0.003087\n",
      "\tTraining batch 15, Images: 480/1000 Total_loss: 0.05258683441206813, Loss: 0.004210\n",
      "\tTraining batch 16, Images: 512/1000 Total_loss: 0.05717287538573146, Loss: 0.004586\n",
      "\tTraining batch 17, Images: 544/1000 Total_loss: 0.06048343167640269, Loss: 0.003311\n",
      "\tTraining batch 18, Images: 576/1000 Total_loss: 0.06371506373398006, Loss: 0.003232\n",
      "\tTraining batch 19, Images: 608/1000 Total_loss: 0.06742940261028707, Loss: 0.003714\n",
      "\tTraining batch 20, Images: 640/1000 Total_loss: 0.07035429170355201, Loss: 0.002925\n",
      "\tTraining batch 21, Images: 672/1000 Total_loss: 0.07404996361583471, Loss: 0.003696\n",
      "\tTraining batch 22, Images: 704/1000 Total_loss: 0.07667977828532457, Loss: 0.002630\n",
      "\tTraining batch 23, Images: 736/1000 Total_loss: 0.0802650866098702, Loss: 0.003585\n",
      "\tTraining batch 24, Images: 768/1000 Total_loss: 0.0834806312341243, Loss: 0.003216\n",
      "\tTraining batch 25, Images: 800/1000 Total_loss: 0.08691751142032444, Loss: 0.003437\n",
      "\tTraining batch 26, Images: 832/1000 Total_loss: 0.09015655401162803, Loss: 0.003239\n",
      "\tTraining batch 27, Images: 864/1000 Total_loss: 0.09456194774247706, Loss: 0.004405\n",
      "\tTraining batch 28, Images: 896/1000 Total_loss: 0.09775897138752043, Loss: 0.003197\n",
      "\tTraining batch 29, Images: 928/1000 Total_loss: 0.10150902881287038, Loss: 0.003750\n",
      "\tTraining batch 30, Images: 960/1000 Total_loss: 0.10487340553663671, Loss: 0.003364\n",
      "\tTraining batch 31, Images: 992/1000 Total_loss: 0.10864897049032152, Loss: 0.003776\n",
      "\tTraining batch 32, Images: 1024/1000 Total_loss: 0.11380402534268796, Loss: 0.005155\n",
      "Average loss for Training set 20: 0.003556\n",
      "Average loss for Validation set 20: 0.002791\n",
      "\n",
      "Epoch #  21\n",
      "\tTraining batch 1, Images: 32/1000 Total_loss: 0.0034468264784663916, Loss: 0.003447\n",
      "\tTraining batch 2, Images: 64/1000 Total_loss: 0.007084931945428252, Loss: 0.003638\n",
      "\tTraining batch 3, Images: 96/1000 Total_loss: 0.010212977649644017, Loss: 0.003128\n",
      "\tTraining batch 4, Images: 128/1000 Total_loss: 0.013881367398425937, Loss: 0.003668\n",
      "\tTraining batch 5, Images: 160/1000 Total_loss: 0.01741527416743338, Loss: 0.003534\n",
      "\tTraining batch 6, Images: 192/1000 Total_loss: 0.02162507572211325, Loss: 0.004210\n",
      "\tTraining batch 7, Images: 224/1000 Total_loss: 0.024681800045073032, Loss: 0.003057\n",
      "\tTraining batch 8, Images: 256/1000 Total_loss: 0.027684969594702125, Loss: 0.003003\n",
      "\tTraining batch 9, Images: 288/1000 Total_loss: 0.030417283531278372, Loss: 0.002732\n",
      "\tTraining batch 10, Images: 320/1000 Total_loss: 0.03393983421847224, Loss: 0.003523\n",
      "\tTraining batch 11, Images: 352/1000 Total_loss: 0.036766334902495146, Loss: 0.002827\n",
      "\tTraining batch 12, Images: 384/1000 Total_loss: 0.04184275586158037, Loss: 0.005076\n",
      "\tTraining batch 13, Images: 416/1000 Total_loss: 0.04516148939728737, Loss: 0.003319\n",
      "\tTraining batch 14, Images: 448/1000 Total_loss: 0.04785968619398773, Loss: 0.002698\n",
      "\tTraining batch 15, Images: 480/1000 Total_loss: 0.05064740520901978, Loss: 0.002788\n",
      "\tTraining batch 16, Images: 512/1000 Total_loss: 0.0547565792221576, Loss: 0.004109\n",
      "\tTraining batch 17, Images: 544/1000 Total_loss: 0.056849285028874874, Loss: 0.002093\n",
      "\tTraining batch 18, Images: 576/1000 Total_loss: 0.05986687960103154, Loss: 0.003018\n",
      "\tTraining batch 19, Images: 608/1000 Total_loss: 0.06333579123020172, Loss: 0.003469\n",
      "\tTraining batch 20, Images: 640/1000 Total_loss: 0.06637353706173599, Loss: 0.003038\n",
      "\tTraining batch 21, Images: 672/1000 Total_loss: 0.06984005705453455, Loss: 0.003467\n",
      "\tTraining batch 22, Images: 704/1000 Total_loss: 0.07261845818720758, Loss: 0.002778\n",
      "\tTraining batch 23, Images: 736/1000 Total_loss: 0.07566345040686429, Loss: 0.003045\n",
      "\tTraining batch 24, Images: 768/1000 Total_loss: 0.07875654264353216, Loss: 0.003093\n",
      "\tTraining batch 25, Images: 800/1000 Total_loss: 0.08146901102736592, Loss: 0.002712\n",
      "\tTraining batch 26, Images: 832/1000 Total_loss: 0.08356354106217623, Loss: 0.002095\n",
      "\tTraining batch 27, Images: 864/1000 Total_loss: 0.08639508578926325, Loss: 0.002832\n",
      "\tTraining batch 28, Images: 896/1000 Total_loss: 0.0900076744146645, Loss: 0.003613\n",
      "\tTraining batch 29, Images: 928/1000 Total_loss: 0.09183214022777975, Loss: 0.001824\n",
      "\tTraining batch 30, Images: 960/1000 Total_loss: 0.09561943681910634, Loss: 0.003787\n",
      "\tTraining batch 31, Images: 992/1000 Total_loss: 0.0985720269382, Loss: 0.002953\n",
      "\tTraining batch 32, Images: 1024/1000 Total_loss: 0.10108313942328095, Loss: 0.002511\n",
      "Average loss for Training set 21: 0.003159\n",
      "Average loss for Validation set 21: 0.002374\n",
      "\n",
      "Epoch #  22\n",
      "\tTraining batch 1, Images: 32/1000 Total_loss: 0.003521644277498126, Loss: 0.003522\n",
      "\tTraining batch 2, Images: 64/1000 Total_loss: 0.007135711144655943, Loss: 0.003614\n",
      "\tTraining batch 3, Images: 96/1000 Total_loss: 0.010057014413177967, Loss: 0.002921\n",
      "\tTraining batch 4, Images: 128/1000 Total_loss: 0.013956705573946238, Loss: 0.003900\n",
      "\tTraining batch 5, Images: 160/1000 Total_loss: 0.01667993632145226, Loss: 0.002723\n",
      "\tTraining batch 6, Images: 192/1000 Total_loss: 0.019335769349709153, Loss: 0.002656\n",
      "\tTraining batch 7, Images: 224/1000 Total_loss: 0.0227928280364722, Loss: 0.003457\n",
      "\tTraining batch 8, Images: 256/1000 Total_loss: 0.02582199382595718, Loss: 0.003029\n",
      "\tTraining batch 9, Images: 288/1000 Total_loss: 0.028527877060696483, Loss: 0.002706\n",
      "\tTraining batch 10, Images: 320/1000 Total_loss: 0.03259633178822696, Loss: 0.004068\n",
      "\tTraining batch 11, Images: 352/1000 Total_loss: 0.03668610914610326, Loss: 0.004090\n",
      "\tTraining batch 12, Images: 384/1000 Total_loss: 0.039762115804478526, Loss: 0.003076\n",
      "\tTraining batch 13, Images: 416/1000 Total_loss: 0.04326721536926925, Loss: 0.003505\n",
      "\tTraining batch 14, Images: 448/1000 Total_loss: 0.04806191776879132, Loss: 0.004795\n",
      "\tTraining batch 15, Images: 480/1000 Total_loss: 0.051403414690867066, Loss: 0.003341\n",
      "\tTraining batch 16, Images: 512/1000 Total_loss: 0.05403720703907311, Loss: 0.002634\n",
      "\tTraining batch 17, Images: 544/1000 Total_loss: 0.05706012528389692, Loss: 0.003023\n",
      "\tTraining batch 18, Images: 576/1000 Total_loss: 0.05960953398607671, Loss: 0.002549\n",
      "\tTraining batch 19, Images: 608/1000 Total_loss: 0.06184167298488319, Loss: 0.002232\n",
      "\tTraining batch 20, Images: 640/1000 Total_loss: 0.0645922846160829, Loss: 0.002751\n",
      "\tTraining batch 21, Images: 672/1000 Total_loss: 0.06817230232991278, Loss: 0.003580\n",
      "\tTraining batch 22, Images: 704/1000 Total_loss: 0.07101431372575462, Loss: 0.002842\n",
      "\tTraining batch 23, Images: 736/1000 Total_loss: 0.07371781347319484, Loss: 0.002703\n",
      "\tTraining batch 24, Images: 768/1000 Total_loss: 0.07681221282109618, Loss: 0.003094\n",
      "\tTraining batch 25, Images: 800/1000 Total_loss: 0.07945809164084494, Loss: 0.002646\n",
      "\tTraining batch 26, Images: 832/1000 Total_loss: 0.0821351835038513, Loss: 0.002677\n",
      "\tTraining batch 27, Images: 864/1000 Total_loss: 0.08547201775945723, Loss: 0.003337\n",
      "\tTraining batch 28, Images: 896/1000 Total_loss: 0.08909348212182522, Loss: 0.003621\n",
      "\tTraining batch 29, Images: 928/1000 Total_loss: 0.09145993902347982, Loss: 0.002366\n",
      "\tTraining batch 30, Images: 960/1000 Total_loss: 0.09459733194671571, Loss: 0.003137\n",
      "\tTraining batch 31, Images: 992/1000 Total_loss: 0.0971817469689995, Loss: 0.002584\n",
      "\tTraining batch 32, Images: 1024/1000 Total_loss: 0.10035916697233915, Loss: 0.003177\n",
      "Average loss for Training set 22: 0.003136\n",
      "Average loss for Validation set 22: 0.002260\n",
      "\n",
      "Epoch #  23\n",
      "\tTraining batch 1, Images: 32/1000 Total_loss: 0.003098640125244856, Loss: 0.003099\n",
      "\tTraining batch 2, Images: 64/1000 Total_loss: 0.00550236226990819, Loss: 0.002404\n",
      "\tTraining batch 3, Images: 96/1000 Total_loss: 0.009753940161317587, Loss: 0.004252\n",
      "\tTraining batch 4, Images: 128/1000 Total_loss: 0.012932903598994017, Loss: 0.003179\n",
      "\tTraining batch 5, Images: 160/1000 Total_loss: 0.015693210996687412, Loss: 0.002760\n",
      "\tTraining batch 6, Images: 192/1000 Total_loss: 0.018156093079596758, Loss: 0.002463\n",
      "\tTraining batch 7, Images: 224/1000 Total_loss: 0.020702789071947336, Loss: 0.002547\n",
      "\tTraining batch 8, Images: 256/1000 Total_loss: 0.023343880893662572, Loss: 0.002641\n",
      "\tTraining batch 9, Images: 288/1000 Total_loss: 0.02595486887730658, Loss: 0.002611\n",
      "\tTraining batch 10, Images: 320/1000 Total_loss: 0.028949255123734474, Loss: 0.002994\n",
      "\tTraining batch 11, Images: 352/1000 Total_loss: 0.03179512871429324, Loss: 0.002846\n",
      "\tTraining batch 12, Images: 384/1000 Total_loss: 0.03440130059607327, Loss: 0.002606\n",
      "\tTraining batch 13, Images: 416/1000 Total_loss: 0.0378195324447006, Loss: 0.003418\n",
      "\tTraining batch 14, Images: 448/1000 Total_loss: 0.040809859056025743, Loss: 0.002990\n",
      "\tTraining batch 15, Images: 480/1000 Total_loss: 0.04321029456332326, Loss: 0.002400\n",
      "\tTraining batch 16, Images: 512/1000 Total_loss: 0.04504864604678005, Loss: 0.001838\n",
      "\tTraining batch 17, Images: 544/1000 Total_loss: 0.048210992594249547, Loss: 0.003162\n",
      "\tTraining batch 18, Images: 576/1000 Total_loss: 0.05118677008431405, Loss: 0.002976\n",
      "\tTraining batch 19, Images: 608/1000 Total_loss: 0.05352895881514996, Loss: 0.002342\n",
      "\tTraining batch 20, Images: 640/1000 Total_loss: 0.05704469105694443, Loss: 0.003516\n",
      "\tTraining batch 21, Images: 672/1000 Total_loss: 0.06035029364284128, Loss: 0.003306\n",
      "\tTraining batch 22, Images: 704/1000 Total_loss: 0.06320306600537151, Loss: 0.002853\n",
      "\tTraining batch 23, Images: 736/1000 Total_loss: 0.06705142825376242, Loss: 0.003848\n",
      "\tTraining batch 24, Images: 768/1000 Total_loss: 0.0700352800777182, Loss: 0.002984\n",
      "\tTraining batch 25, Images: 800/1000 Total_loss: 0.07339589984621853, Loss: 0.003361\n",
      "\tTraining batch 26, Images: 832/1000 Total_loss: 0.07512493000831455, Loss: 0.001729\n",
      "\tTraining batch 27, Images: 864/1000 Total_loss: 0.07902471430134028, Loss: 0.003900\n",
      "\tTraining batch 28, Images: 896/1000 Total_loss: 0.08220361068379134, Loss: 0.003179\n",
      "\tTraining batch 29, Images: 928/1000 Total_loss: 0.08542020304594189, Loss: 0.003217\n",
      "\tTraining batch 30, Images: 960/1000 Total_loss: 0.08787778404075652, Loss: 0.002458\n",
      "\tTraining batch 31, Images: 992/1000 Total_loss: 0.09082629799377173, Loss: 0.002949\n",
      "\tTraining batch 32, Images: 1024/1000 Total_loss: 0.09328652697149664, Loss: 0.002460\n",
      "Average loss for Training set 23: 0.002915\n",
      "Average loss for Validation set 23: 0.002812\n",
      "\n",
      "Epoch #  24\n",
      "\tTraining batch 1, Images: 32/1000 Total_loss: 0.0034496132284402847, Loss: 0.003450\n",
      "\tTraining batch 2, Images: 64/1000 Total_loss: 0.006250958424061537, Loss: 0.002801\n",
      "\tTraining batch 3, Images: 96/1000 Total_loss: 0.009270518086850643, Loss: 0.003020\n",
      "\tTraining batch 4, Images: 128/1000 Total_loss: 0.011867276392877102, Loss: 0.002597\n",
      "\tTraining batch 5, Images: 160/1000 Total_loss: 0.014801040990278125, Loss: 0.002934\n",
      "\tTraining batch 6, Images: 192/1000 Total_loss: 0.018268838990479708, Loss: 0.003468\n",
      "\tTraining batch 7, Images: 224/1000 Total_loss: 0.021946652326732874, Loss: 0.003678\n",
      "\tTraining batch 8, Images: 256/1000 Total_loss: 0.02466580574400723, Loss: 0.002719\n",
      "\tTraining batch 9, Images: 288/1000 Total_loss: 0.026716918917372823, Loss: 0.002051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 10, Images: 320/1000 Total_loss: 0.029740408761426806, Loss: 0.003023\n",
      "\tTraining batch 11, Images: 352/1000 Total_loss: 0.03304095147177577, Loss: 0.003301\n",
      "\tTraining batch 12, Images: 384/1000 Total_loss: 0.03577021788805723, Loss: 0.002729\n",
      "\tTraining batch 13, Images: 416/1000 Total_loss: 0.03861308703199029, Loss: 0.002843\n",
      "\tTraining batch 14, Images: 448/1000 Total_loss: 0.04224020289257169, Loss: 0.003627\n",
      "\tTraining batch 15, Images: 480/1000 Total_loss: 0.045343044213950634, Loss: 0.003103\n",
      "\tTraining batch 16, Images: 512/1000 Total_loss: 0.04821932315826416, Loss: 0.002876\n",
      "\tTraining batch 17, Images: 544/1000 Total_loss: 0.05187062150798738, Loss: 0.003651\n",
      "\tTraining batch 18, Images: 576/1000 Total_loss: 0.053844320587813854, Loss: 0.001974\n",
      "\tTraining batch 19, Images: 608/1000 Total_loss: 0.056559166638180614, Loss: 0.002715\n",
      "\tTraining batch 20, Images: 640/1000 Total_loss: 0.059045698028057814, Loss: 0.002487\n",
      "\tTraining batch 21, Images: 672/1000 Total_loss: 0.06184138567186892, Loss: 0.002796\n",
      "\tTraining batch 22, Images: 704/1000 Total_loss: 0.06504406104795635, Loss: 0.003203\n",
      "\tTraining batch 23, Images: 736/1000 Total_loss: 0.06804543198086321, Loss: 0.003001\n",
      "\tTraining batch 24, Images: 768/1000 Total_loss: 0.07082364405505359, Loss: 0.002778\n",
      "\tTraining batch 25, Images: 800/1000 Total_loss: 0.0728821421507746, Loss: 0.002058\n",
      "\tTraining batch 26, Images: 832/1000 Total_loss: 0.07563271932303905, Loss: 0.002751\n",
      "\tTraining batch 27, Images: 864/1000 Total_loss: 0.07842605654150248, Loss: 0.002793\n",
      "\tTraining batch 28, Images: 896/1000 Total_loss: 0.08070823643356562, Loss: 0.002282\n",
      "\tTraining batch 29, Images: 928/1000 Total_loss: 0.08385046082548797, Loss: 0.003142\n",
      "\tTraining batch 30, Images: 960/1000 Total_loss: 0.08599929441697896, Loss: 0.002149\n",
      "\tTraining batch 31, Images: 992/1000 Total_loss: 0.0886528070550412, Loss: 0.002654\n",
      "\tTraining batch 32, Images: 1024/1000 Total_loss: 0.0902632069773972, Loss: 0.001610\n",
      "Average loss for Training set 24: 0.002821\n",
      "Average loss for Validation set 24: 0.002040\n",
      "\n",
      "Epoch #  25\n",
      "\tTraining batch 1, Images: 32/1000 Total_loss: 0.0030529494397342205, Loss: 0.003053\n",
      "\tTraining batch 2, Images: 64/1000 Total_loss: 0.005359604023396969, Loss: 0.002307\n",
      "\tTraining batch 3, Images: 96/1000 Total_loss: 0.008027826435863972, Loss: 0.002668\n",
      "\tTraining batch 4, Images: 128/1000 Total_loss: 0.011508978670462966, Loss: 0.003481\n",
      "\tTraining batch 5, Images: 160/1000 Total_loss: 0.014107057126238942, Loss: 0.002598\n",
      "\tTraining batch 6, Images: 192/1000 Total_loss: 0.0173462787643075, Loss: 0.003239\n",
      "\tTraining batch 7, Images: 224/1000 Total_loss: 0.02015252923592925, Loss: 0.002806\n",
      "\tTraining batch 8, Images: 256/1000 Total_loss: 0.02329030097462237, Loss: 0.003138\n",
      "\tTraining batch 9, Images: 288/1000 Total_loss: 0.0263904572930187, Loss: 0.003100\n",
      "\tTraining batch 10, Images: 320/1000 Total_loss: 0.028660311130806804, Loss: 0.002270\n",
      "\tTraining batch 11, Images: 352/1000 Total_loss: 0.03062154189683497, Loss: 0.001961\n",
      "\tTraining batch 12, Images: 384/1000 Total_loss: 0.032846532529219985, Loss: 0.002225\n",
      "\tTraining batch 13, Images: 416/1000 Total_loss: 0.03537651966325939, Loss: 0.002530\n",
      "\tTraining batch 14, Images: 448/1000 Total_loss: 0.037888554157689214, Loss: 0.002512\n",
      "\tTraining batch 15, Images: 480/1000 Total_loss: 0.04077490675263107, Loss: 0.002886\n",
      "\tTraining batch 16, Images: 512/1000 Total_loss: 0.043581615667790174, Loss: 0.002807\n",
      "\tTraining batch 17, Images: 544/1000 Total_loss: 0.045810189098119736, Loss: 0.002229\n",
      "\tTraining batch 18, Images: 576/1000 Total_loss: 0.04897533170878887, Loss: 0.003165\n",
      "\tTraining batch 19, Images: 608/1000 Total_loss: 0.05232479562982917, Loss: 0.003349\n",
      "\tTraining batch 20, Images: 640/1000 Total_loss: 0.05471238144673407, Loss: 0.002388\n",
      "\tTraining batch 21, Images: 672/1000 Total_loss: 0.057695986004546285, Loss: 0.002984\n",
      "\tTraining batch 22, Images: 704/1000 Total_loss: 0.06051347521133721, Loss: 0.002817\n",
      "\tTraining batch 23, Images: 736/1000 Total_loss: 0.06249045790173113, Loss: 0.001977\n",
      "\tTraining batch 24, Images: 768/1000 Total_loss: 0.06546532269567251, Loss: 0.002975\n",
      "\tTraining batch 25, Images: 800/1000 Total_loss: 0.06854403438046575, Loss: 0.003079\n",
      "\tTraining batch 26, Images: 832/1000 Total_loss: 0.07052085641771555, Loss: 0.001977\n",
      "\tTraining batch 27, Images: 864/1000 Total_loss: 0.07319226465187967, Loss: 0.002671\n",
      "\tTraining batch 28, Images: 896/1000 Total_loss: 0.07467297813855112, Loss: 0.001481\n",
      "\tTraining batch 29, Images: 928/1000 Total_loss: 0.07685369229875505, Loss: 0.002181\n",
      "\tTraining batch 30, Images: 960/1000 Total_loss: 0.07936002011410892, Loss: 0.002506\n",
      "\tTraining batch 31, Images: 992/1000 Total_loss: 0.08230417291633785, Loss: 0.002944\n",
      "\tTraining batch 32, Images: 1024/1000 Total_loss: 0.08477310533635318, Loss: 0.002469\n",
      "Average loss for Training set 25: 0.002649\n",
      "Average loss for Validation set 25: 0.002062\n",
      "\n",
      "Epoch #  26\n",
      "\tTraining batch 1, Images: 32/1000 Total_loss: 0.002793774474412203, Loss: 0.002794\n",
      "\tTraining batch 2, Images: 64/1000 Total_loss: 0.005784137407317758, Loss: 0.002990\n",
      "\tTraining batch 3, Images: 96/1000 Total_loss: 0.008542104857042432, Loss: 0.002758\n",
      "\tTraining batch 4, Images: 128/1000 Total_loss: 0.01149577903561294, Loss: 0.002954\n",
      "\tTraining batch 5, Images: 160/1000 Total_loss: 0.01402193563990295, Loss: 0.002526\n",
      "\tTraining batch 6, Images: 192/1000 Total_loss: 0.016645030584186316, Loss: 0.002623\n",
      "\tTraining batch 7, Images: 224/1000 Total_loss: 0.019192931009456515, Loss: 0.002548\n",
      "\tTraining batch 8, Images: 256/1000 Total_loss: 0.021829355042427778, Loss: 0.002636\n",
      "\tTraining batch 9, Images: 288/1000 Total_loss: 0.024296993855386972, Loss: 0.002468\n",
      "\tTraining batch 10, Images: 320/1000 Total_loss: 0.027357748011127114, Loss: 0.003061\n",
      "\tTraining batch 11, Images: 352/1000 Total_loss: 0.030507086077705026, Loss: 0.003149\n",
      "\tTraining batch 12, Images: 384/1000 Total_loss: 0.033661469584330916, Loss: 0.003154\n",
      "\tTraining batch 13, Images: 416/1000 Total_loss: 0.03621087805368006, Loss: 0.002549\n",
      "\tTraining batch 14, Images: 448/1000 Total_loss: 0.03867977228946984, Loss: 0.002469\n",
      "\tTraining batch 15, Images: 480/1000 Total_loss: 0.04150261916220188, Loss: 0.002823\n",
      "\tTraining batch 16, Images: 512/1000 Total_loss: 0.04433204676024616, Loss: 0.002829\n",
      "\tTraining batch 17, Images: 544/1000 Total_loss: 0.04681160976178944, Loss: 0.002480\n",
      "\tTraining batch 18, Images: 576/1000 Total_loss: 0.05002620629966259, Loss: 0.003215\n",
      "\tTraining batch 19, Images: 608/1000 Total_loss: 0.05283312499523163, Loss: 0.002807\n",
      "\tTraining batch 20, Images: 640/1000 Total_loss: 0.05540911480784416, Loss: 0.002576\n",
      "\tTraining batch 21, Images: 672/1000 Total_loss: 0.05813515791669488, Loss: 0.002726\n",
      "\tTraining batch 22, Images: 704/1000 Total_loss: 0.060657653491944075, Loss: 0.002522\n",
      "\tTraining batch 23, Images: 736/1000 Total_loss: 0.06349410396069288, Loss: 0.002836\n",
      "\tTraining batch 24, Images: 768/1000 Total_loss: 0.06610093242488801, Loss: 0.002607\n",
      "\tTraining batch 25, Images: 800/1000 Total_loss: 0.06942722876556218, Loss: 0.003326\n",
      "\tTraining batch 26, Images: 832/1000 Total_loss: 0.07290863920934498, Loss: 0.003481\n",
      "\tTraining batch 27, Images: 864/1000 Total_loss: 0.07561107841320336, Loss: 0.002702\n",
      "\tTraining batch 28, Images: 896/1000 Total_loss: 0.07828419585712254, Loss: 0.002673\n",
      "\tTraining batch 29, Images: 928/1000 Total_loss: 0.08202121988870203, Loss: 0.003737\n",
      "\tTraining batch 30, Images: 960/1000 Total_loss: 0.08526707743294537, Loss: 0.003246\n",
      "\tTraining batch 31, Images: 992/1000 Total_loss: 0.08881815918721259, Loss: 0.003551\n",
      "\tTraining batch 32, Images: 1024/1000 Total_loss: 0.09213196323253214, Loss: 0.003314\n",
      "Average loss for Training set 26: 0.002879\n",
      "Average loss for Validation set 26: 0.002023\n",
      "\n",
      "Epoch #  27\n",
      "\tTraining batch 1, Images: 32/1000 Total_loss: 0.003471147269010544, Loss: 0.003471\n",
      "\tTraining batch 2, Images: 64/1000 Total_loss: 0.005878457333892584, Loss: 0.002407\n",
      "\tTraining batch 3, Images: 96/1000 Total_loss: 0.009358497569337487, Loss: 0.003480\n",
      "\tTraining batch 4, Images: 128/1000 Total_loss: 0.011642582481727004, Loss: 0.002284\n",
      "\tTraining batch 5, Images: 160/1000 Total_loss: 0.014149628346785903, Loss: 0.002507\n",
      "\tTraining batch 6, Images: 192/1000 Total_loss: 0.016735276905819774, Loss: 0.002586\n",
      "\tTraining batch 7, Images: 224/1000 Total_loss: 0.019647040171548724, Loss: 0.002912\n",
      "\tTraining batch 8, Images: 256/1000 Total_loss: 0.022430497454479337, Loss: 0.002783\n",
      "\tTraining batch 9, Images: 288/1000 Total_loss: 0.024911017389968038, Loss: 0.002481\n",
      "\tTraining batch 10, Images: 320/1000 Total_loss: 0.028469428420066833, Loss: 0.003558\n",
      "\tTraining batch 11, Images: 352/1000 Total_loss: 0.03137547220103443, Loss: 0.002906\n",
      "\tTraining batch 12, Images: 384/1000 Total_loss: 0.03400339325889945, Loss: 0.002628\n",
      "\tTraining batch 13, Images: 416/1000 Total_loss: 0.03658783994615078, Loss: 0.002584\n",
      "\tTraining batch 14, Images: 448/1000 Total_loss: 0.0400654177647084, Loss: 0.003478\n",
      "\tTraining batch 15, Images: 480/1000 Total_loss: 0.04209500132128596, Loss: 0.002030\n",
      "\tTraining batch 16, Images: 512/1000 Total_loss: 0.044308511540293694, Loss: 0.002214\n",
      "\tTraining batch 17, Images: 544/1000 Total_loss: 0.046875441214069724, Loss: 0.002567\n",
      "\tTraining batch 18, Images: 576/1000 Total_loss: 0.04974490823224187, Loss: 0.002869\n",
      "\tTraining batch 19, Images: 608/1000 Total_loss: 0.05205562640912831, Loss: 0.002311\n",
      "\tTraining batch 20, Images: 640/1000 Total_loss: 0.05464191106148064, Loss: 0.002586\n",
      "\tTraining batch 21, Images: 672/1000 Total_loss: 0.058211782947182655, Loss: 0.003570\n",
      "\tTraining batch 22, Images: 704/1000 Total_loss: 0.061479492811486125, Loss: 0.003268\n",
      "\tTraining batch 23, Images: 736/1000 Total_loss: 0.06455707806162536, Loss: 0.003078\n",
      "\tTraining batch 24, Images: 768/1000 Total_loss: 0.06669912254437804, Loss: 0.002142\n",
      "\tTraining batch 25, Images: 800/1000 Total_loss: 0.06963508785702288, Loss: 0.002936\n",
      "\tTraining batch 26, Images: 832/1000 Total_loss: 0.07249741535633802, Loss: 0.002862\n",
      "\tTraining batch 27, Images: 864/1000 Total_loss: 0.07474978733807802, Loss: 0.002252\n",
      "\tTraining batch 28, Images: 896/1000 Total_loss: 0.07670425157994032, Loss: 0.001954\n",
      "\tTraining batch 29, Images: 928/1000 Total_loss: 0.0797483881469816, Loss: 0.003044\n",
      "\tTraining batch 30, Images: 960/1000 Total_loss: 0.0834470575209707, Loss: 0.003699\n",
      "\tTraining batch 31, Images: 992/1000 Total_loss: 0.08632044564001262, Loss: 0.002873\n",
      "\tTraining batch 32, Images: 1024/1000 Total_loss: 0.08980153012089431, Loss: 0.003481\n",
      "Average loss for Training set 27: 0.002806\n",
      "Average loss for Validation set 27: 0.003004\n",
      "\n",
      "Epoch #  28\n",
      "\tTraining batch 1, Images: 32/1000 Total_loss: 0.004059604834765196, Loss: 0.004060\n",
      "\tTraining batch 2, Images: 64/1000 Total_loss: 0.006637136451900005, Loss: 0.002578\n",
      "\tTraining batch 3, Images: 96/1000 Total_loss: 0.009359353920444846, Loss: 0.002722\n",
      "\tTraining batch 4, Images: 128/1000 Total_loss: 0.012533566681668162, Loss: 0.003174\n",
      "\tTraining batch 5, Images: 160/1000 Total_loss: 0.015319063561037183, Loss: 0.002785\n",
      "\tTraining batch 6, Images: 192/1000 Total_loss: 0.019191819475963712, Loss: 0.003873\n",
      "\tTraining batch 7, Images: 224/1000 Total_loss: 0.022631999105215073, Loss: 0.003440\n",
      "\tTraining batch 8, Images: 256/1000 Total_loss: 0.026006595231592655, Loss: 0.003375\n",
      "\tTraining batch 9, Images: 288/1000 Total_loss: 0.029280574759468436, Loss: 0.003274\n",
      "\tTraining batch 10, Images: 320/1000 Total_loss: 0.032425348879769444, Loss: 0.003145\n",
      "\tTraining batch 11, Images: 352/1000 Total_loss: 0.03526788321323693, Loss: 0.002843\n",
      "\tTraining batch 12, Images: 384/1000 Total_loss: 0.0374446795322001, Loss: 0.002177\n",
      "\tTraining batch 13, Images: 416/1000 Total_loss: 0.04060561605729163, Loss: 0.003161\n",
      "\tTraining batch 14, Images: 448/1000 Total_loss: 0.04407611885108054, Loss: 0.003471\n",
      "\tTraining batch 15, Images: 480/1000 Total_loss: 0.04699373967014253, Loss: 0.002918\n",
      "\tTraining batch 16, Images: 512/1000 Total_loss: 0.04950988735072315, Loss: 0.002516\n",
      "\tTraining batch 17, Images: 544/1000 Total_loss: 0.05155498092062771, Loss: 0.002045\n",
      "\tTraining batch 18, Images: 576/1000 Total_loss: 0.05488133430480957, Loss: 0.003326\n",
      "\tTraining batch 19, Images: 608/1000 Total_loss: 0.057137338910251856, Loss: 0.002256\n",
      "\tTraining batch 20, Images: 640/1000 Total_loss: 0.060659626964479685, Loss: 0.003522\n",
      "\tTraining batch 21, Images: 672/1000 Total_loss: 0.06287106685340405, Loss: 0.002211\n",
      "\tTraining batch 22, Images: 704/1000 Total_loss: 0.06507816631346941, Loss: 0.002207\n",
      "\tTraining batch 23, Images: 736/1000 Total_loss: 0.0671674576587975, Loss: 0.002089\n",
      "\tTraining batch 24, Images: 768/1000 Total_loss: 0.06983495433814824, Loss: 0.002667\n",
      "\tTraining batch 25, Images: 800/1000 Total_loss: 0.07261686865240335, Loss: 0.002782\n",
      "\tTraining batch 26, Images: 832/1000 Total_loss: 0.07467082887887955, Loss: 0.002054\n",
      "\tTraining batch 27, Images: 864/1000 Total_loss: 0.07837823475711048, Loss: 0.003707\n",
      "\tTraining batch 28, Images: 896/1000 Total_loss: 0.08061639382503927, Loss: 0.002238\n",
      "\tTraining batch 29, Images: 928/1000 Total_loss: 0.08298665494658053, Loss: 0.002370\n",
      "\tTraining batch 30, Images: 960/1000 Total_loss: 0.08496002946048975, Loss: 0.001973\n",
      "\tTraining batch 31, Images: 992/1000 Total_loss: 0.0874062953516841, Loss: 0.002446\n",
      "\tTraining batch 32, Images: 1024/1000 Total_loss: 0.08995533175766468, Loss: 0.002549\n",
      "Average loss for Training set 28: 0.002811\n",
      "Average loss for Validation set 28: 0.002013\n",
      "\n",
      "Epoch #  29\n",
      "\tTraining batch 1, Images: 32/1000 Total_loss: 0.0018826061859726906, Loss: 0.001883\n",
      "\tTraining batch 2, Images: 64/1000 Total_loss: 0.003772851312533021, Loss: 0.001890\n",
      "\tTraining batch 3, Images: 96/1000 Total_loss: 0.005562006728723645, Loss: 0.001789\n",
      "\tTraining batch 4, Images: 128/1000 Total_loss: 0.007612875429913402, Loss: 0.002051\n",
      "\tTraining batch 5, Images: 160/1000 Total_loss: 0.00993634620681405, Loss: 0.002323\n",
      "\tTraining batch 6, Images: 192/1000 Total_loss: 0.011743410839699209, Loss: 0.001807\n",
      "\tTraining batch 7, Images: 224/1000 Total_loss: 0.014825245481915772, Loss: 0.003082\n",
      "\tTraining batch 8, Images: 256/1000 Total_loss: 0.017044890788383782, Loss: 0.002220\n",
      "\tTraining batch 9, Images: 288/1000 Total_loss: 0.019035989535041153, Loss: 0.001991\n",
      "\tTraining batch 10, Images: 320/1000 Total_loss: 0.02155410370323807, Loss: 0.002518\n",
      "\tTraining batch 11, Images: 352/1000 Total_loss: 0.023982812999747694, Loss: 0.002429\n",
      "\tTraining batch 12, Images: 384/1000 Total_loss: 0.026983242598362267, Loss: 0.003000\n",
      "\tTraining batch 13, Images: 416/1000 Total_loss: 0.030090523534454405, Loss: 0.003107\n",
      "\tTraining batch 14, Images: 448/1000 Total_loss: 0.03318441694136709, Loss: 0.003094\n",
      "\tTraining batch 15, Images: 480/1000 Total_loss: 0.03543843363877386, Loss: 0.002254\n",
      "\tTraining batch 16, Images: 512/1000 Total_loss: 0.038141216966323555, Loss: 0.002703\n",
      "\tTraining batch 17, Images: 544/1000 Total_loss: 0.04177225811872631, Loss: 0.003631\n",
      "\tTraining batch 18, Images: 576/1000 Total_loss: 0.0440820517251268, Loss: 0.002310\n",
      "\tTraining batch 19, Images: 608/1000 Total_loss: 0.046425553620792925, Loss: 0.002344\n",
      "\tTraining batch 20, Images: 640/1000 Total_loss: 0.0490412242943421, Loss: 0.002616\n",
      "\tTraining batch 21, Images: 672/1000 Total_loss: 0.05070348642766476, Loss: 0.001662\n",
      "\tTraining batch 22, Images: 704/1000 Total_loss: 0.05377132631838322, Loss: 0.003068\n",
      "\tTraining batch 23, Images: 736/1000 Total_loss: 0.055928942281752825, Loss: 0.002158\n",
      "\tTraining batch 24, Images: 768/1000 Total_loss: 0.05853107664734125, Loss: 0.002602\n",
      "\tTraining batch 25, Images: 800/1000 Total_loss: 0.06067284173332155, Loss: 0.002142\n",
      "\tTraining batch 26, Images: 832/1000 Total_loss: 0.06244765187148005, Loss: 0.001775\n",
      "\tTraining batch 27, Images: 864/1000 Total_loss: 0.06554518395569175, Loss: 0.003098\n",
      "\tTraining batch 28, Images: 896/1000 Total_loss: 0.06763576704543084, Loss: 0.002091\n",
      "\tTraining batch 29, Images: 928/1000 Total_loss: 0.07083795720245689, Loss: 0.003202\n",
      "\tTraining batch 30, Images: 960/1000 Total_loss: 0.07388550310861319, Loss: 0.003048\n",
      "\tTraining batch 31, Images: 992/1000 Total_loss: 0.07606913975905627, Loss: 0.002184\n",
      "\tTraining batch 32, Images: 1024/1000 Total_loss: 0.0783152770018205, Loss: 0.002246\n",
      "Average loss for Training set 29: 0.002447\n",
      "Average loss for Validation set 29: 0.002103\n",
      "\n",
      "Epoch #  30\n",
      "\tTraining batch 1, Images: 32/1000 Total_loss: 0.0023755354341119528, Loss: 0.002376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining batch 2, Images: 64/1000 Total_loss: 0.0047929673455655575, Loss: 0.002417\n",
      "\tTraining batch 3, Images: 96/1000 Total_loss: 0.007698295172303915, Loss: 0.002905\n",
      "\tTraining batch 4, Images: 128/1000 Total_loss: 0.010532640852034092, Loss: 0.002834\n",
      "\tTraining batch 5, Images: 160/1000 Total_loss: 0.013260368956252933, Loss: 0.002728\n",
      "\tTraining batch 6, Images: 192/1000 Total_loss: 0.015571783995255828, Loss: 0.002311\n",
      "\tTraining batch 7, Images: 224/1000 Total_loss: 0.01811851537786424, Loss: 0.002547\n",
      "\tTraining batch 8, Images: 256/1000 Total_loss: 0.020951258251443505, Loss: 0.002833\n",
      "\tTraining batch 9, Images: 288/1000 Total_loss: 0.02315811556763947, Loss: 0.002207\n",
      "\tTraining batch 10, Images: 320/1000 Total_loss: 0.026354010915383697, Loss: 0.003196\n",
      "\tTraining batch 11, Images: 352/1000 Total_loss: 0.029310124227777123, Loss: 0.002956\n",
      "\tTraining batch 12, Images: 384/1000 Total_loss: 0.03200458851642907, Loss: 0.002694\n",
      "\tTraining batch 13, Images: 416/1000 Total_loss: 0.03429035912267864, Loss: 0.002286\n",
      "\tTraining batch 14, Images: 448/1000 Total_loss: 0.036653084913268685, Loss: 0.002363\n",
      "\tTraining batch 15, Images: 480/1000 Total_loss: 0.03870732104405761, Loss: 0.002054\n",
      "\tTraining batch 16, Images: 512/1000 Total_loss: 0.04147656657733023, Loss: 0.002769\n",
      "\tTraining batch 17, Images: 544/1000 Total_loss: 0.04384593921713531, Loss: 0.002369\n",
      "\tTraining batch 18, Images: 576/1000 Total_loss: 0.0466776832472533, Loss: 0.002832\n",
      "\tTraining batch 19, Images: 608/1000 Total_loss: 0.04901523748412728, Loss: 0.002338\n",
      "\tTraining batch 20, Images: 640/1000 Total_loss: 0.05254057236015797, Loss: 0.003525\n",
      "\tTraining batch 21, Images: 672/1000 Total_loss: 0.05556498607620597, Loss: 0.003024\n",
      "\tTraining batch 22, Images: 704/1000 Total_loss: 0.05783750046975911, Loss: 0.002273\n",
      "\tTraining batch 23, Images: 736/1000 Total_loss: 0.06153257470577955, Loss: 0.003695\n",
      "\tTraining batch 24, Images: 768/1000 Total_loss: 0.06406648061238229, Loss: 0.002534\n",
      "\tTraining batch 25, Images: 800/1000 Total_loss: 0.0669370957184583, Loss: 0.002871\n",
      "\tTraining batch 26, Images: 832/1000 Total_loss: 0.06916917394846678, Loss: 0.002232\n",
      "\tTraining batch 27, Images: 864/1000 Total_loss: 0.07288380875252187, Loss: 0.003715\n",
      "\tTraining batch 28, Images: 896/1000 Total_loss: 0.07566420594230294, Loss: 0.002780\n",
      "\tTraining batch 29, Images: 928/1000 Total_loss: 0.0790737527422607, Loss: 0.003410\n",
      "\tTraining batch 30, Images: 960/1000 Total_loss: 0.08176787965930998, Loss: 0.002694\n",
      "\tTraining batch 31, Images: 992/1000 Total_loss: 0.08504429925233126, Loss: 0.003276\n",
      "\tTraining batch 32, Images: 1024/1000 Total_loss: 0.08684036077465862, Loss: 0.001796\n",
      "Average loss for Training set 30: 0.002714\n",
      "Average loss for Validation set 30: 0.001897\n"
     ]
    }
   ],
   "source": [
    "# Using two lists (train_losses, validation_losses) containing history of losses \n",
    "# (i.e., loss for each training epoch) for train and validation sets. \n",
    "# If thess are not defined, we define them. Otherwise, the function train_model\n",
    "# updates these two lists (by adding loss values when it is called for further training) \n",
    "# in order to be able to visualize train and validation losses\n",
    "\n",
    "\n",
    "if not 'train_losses' in vars():\n",
    "    train_losses = []\n",
    "if not 'validation_losses' in vars():\n",
    "    validation_losses = []\n",
    "\n",
    "\n",
    "train_model(model, \n",
    "            optimizer,\n",
    "            train_loader,   #train_loader, \n",
    "            validation_loader, \n",
    "            train_losses, \n",
    "            validation_losses,\n",
    "            epochs=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot losses from training process (given to you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_loss len 30\n",
      "Type training_loss <class 'list'>\n",
      "[0.10691430664155632, 0.02289419563021511, 0.02178645064122975, 0.02219472557771951, 0.021588515490293503, 0.0208683317177929, 0.019203111325623468, 0.015094081230927259, 0.012738844059640542, 0.010937373721390031, 0.009639891271945089, 0.008679873033543117, 0.007698492787312716, 0.0063614070968469605, 0.005839564517373219, 0.005443122397991829, 0.0045193517580628395, 0.003722919966094196, 0.0035805099178105593, 0.003556375791958999, 0.00315884810697753, 0.0031362239678855985, 0.00291520396785927, 0.0028207252180436626, 0.002649159541761037, 0.0028791238510166295, 0.0028062978162779473, 0.0028111041174270213, 0.0024473524063068908, 0.002713761274208082]\n",
      "validation_losses len 30\n",
      "[0.02176060391124338, 0.02250105675077066, 0.020775048732757567, 0.02143783333711326, 0.02152558905479964, 0.019956396577035775, 0.016039384090108796, 0.015448603191762231, 0.011271974047122057, 0.009592137183062732, 0.00791329194442369, 0.007252320768893697, 0.0063770636316621675, 0.005613971719139954, 0.0043729836295096905, 0.003860074008698575, 0.0031800841829317506, 0.0032866740776080405, 0.0023961724826222055, 0.0027912701374589234, 0.0023735792423212844, 0.002259854355270363, 0.0028119919788150583, 0.0020397404146933694, 0.002061963255855517, 0.002022859859534947, 0.003003943443472963, 0.0020131304794267636, 0.002102668436637032, 0.001897060855771997]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"training_loss len\", len(train_losses))\n",
    "print(\"Type training_loss\", type(train_losses))\n",
    "print(train_losses)\n",
    "\n",
    "print(\"validation_losses len\", len(validation_losses))\n",
    "print(validation_losses)\n",
    "\n",
    "iteration = np.arange(0., len(train_losses))\n",
    "plt.plot(iteration, train_losses, 'g-', iteration, validation_losses, 'r-')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save/Load Model (given to you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, train_losses, validation_losses, save_dir):\n",
    "    \"\"\"\n",
    "    saving model, train losses, and validation losses\n",
    "    Args:\n",
    "        model              - NN to be saved\n",
    "        train_losses       - history of losses for training dataset\n",
    "        validation_losses  - history of losses for validation dataset\n",
    "        save_dir           - directory where to save the above\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    files = glob.glob(save_dir + '*')\n",
    "    for f in files:\n",
    "        os.remove(f) \n",
    "    \n",
    "    \n",
    "    torch.save(model, save_dir + \"/model.dat\")\n",
    "    \n",
    "    train_losses_f = open(save_dir + \"/train_losses.txt\", \"wt\")\n",
    "    train_losses_f.writelines( \"%.3f\\n\" % item for item in train_losses)\n",
    "    \n",
    "    validation_losses_f = open(save_dir + \"/validation_losses.txt\", \"wt\")\n",
    "    validation_losses_f.writelines( \"%.3f\\n\" % item for item in validation_losses)\n",
    "\n",
    "    return\n",
    "   \n",
    "\n",
    "def load(save_dir):\n",
    "    \"\"\"\n",
    "    loading model, train losses, and validation losses\n",
    "    Args:\n",
    "       save_dir  - dir name from where to load \n",
    "    \"\"\"\n",
    "    \n",
    "    model = torch.load(save_dir + \"/model.dat\") \n",
    "    \n",
    "    train_losses_f = open(save_dir + \"/train_losses.txt\", \"rt\")\n",
    "    train_losses   = train_losses_f.readlines()\n",
    "    train_losses   = [float(num) for num in train_losses]\n",
    "    \n",
    "    validation_losses_f = open(save_dir + \"/validation_losses.txt\", \"rt\")\n",
    "    validation_losses   = validation_losses_f.readlines()\n",
    "    validation_losses   = [float(num) for num in validation_losses]\n",
    "    \n",
    "    return (model, train_losses, validation_losses)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example saving and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda\\lib\\site-packages\\torch\\serialization.py:360: UserWarning: Couldn't retrieve source code for container of type CircleNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# Create a directory, for example \"./saves_12/\", where you place your saved models\n",
    "\n",
    "save(model, train_losses, validation_losses, \"./saves/\")\n",
    "\n",
    "model, train_losses, validation_losses = load(\"./saves/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paint circles on loader images (given to you)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paint_loader_circles(model, loader, out_dir):\n",
    "    \"\"\"\n",
    "    This fucntion receives a model, a loader and an output directory. \n",
    "    For each image in the loader it paints a circle that the model identifies. \n",
    "    The images are saved in the given out_dir diretory. \n",
    "    Args:\n",
    "        model   - network for idneitfying circles\n",
    "        loader  - input data to use \n",
    "        out_dir - ouptut directory name (e.g.: 'draws/'). If directory does not exist, it is created.\n",
    "                  If it exists, its files are deleted.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "                  # (dropout is set to zero)\n",
    "\n",
    "    k = 0\n",
    "    \n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    files = glob.glob(out_dir + '*')\n",
    "    for f in files:\n",
    "        os.remove(f) \n",
    "  \n",
    "        \n",
    "    for data in loader:\n",
    "        # get inputs\n",
    "        inputs = (data['image']).to(device)\n",
    "        labels = (data['labels']).to(device)  # not using \n",
    "        img_fnames = data['fname'] \n",
    "      \n",
    "        \n",
    "        # forward\n",
    "        outputs = model(inputs.float())\n",
    "        curr_batch_size = np.shape(outputs)[0]\n",
    "        image_size = np.shape(inputs[0])  # image_size = [3, w, h]\n",
    "        _, width, height = image_size\n",
    "        assert (width == height)\n",
    "        \n",
    "        for i in range (curr_batch_size): \n",
    "            x0 = (outputs[i, 0].item()) * width\n",
    "            y0 = (1-outputs[i, 1].item()) * height\n",
    "            r  = outputs[i, 2].item() * width #assume width=height here. Otherwise, circle becomes ellipse\n",
    "   \n",
    "            fname = img_fnames[i]\n",
    "            k+=1\n",
    "            print (str(k) + \".   \" + fname)\n",
    "\n",
    "            img = Image.open(fname)\n",
    "            draw = ImageDraw.Draw(img, 'RGBA')\n",
    "    \n",
    "            draw.ellipse((x0 - r, y0 - r, x0 + r ,y0 + r), fill=(160, 64, 0, 90), outline=None)\n",
    "    \n",
    "            img.save(out_dir + fname.split('/')[-1])\n",
    "    \n",
    "        \n",
    "    model.train()  #back to default\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Example of how to paint circles produced by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.   ./validation//images/0000.png\n",
      "2.   ./validation//images/0001.png\n",
      "3.   ./validation//images/0002.png\n",
      "4.   ./validation//images/0003.png\n",
      "5.   ./validation//images/0004.png\n",
      "6.   ./validation//images/0005.png\n",
      "7.   ./validation//images/0006.png\n",
      "8.   ./validation//images/0007.png\n",
      "9.   ./validation//images/0008.png\n",
      "10.   ./validation//images/0009.png\n",
      "11.   ./validation//images/0010.png\n",
      "12.   ./validation//images/0011.png\n",
      "13.   ./validation//images/0012.png\n",
      "14.   ./validation//images/0013.png\n",
      "15.   ./validation//images/0014.png\n",
      "16.   ./validation//images/0015.png\n",
      "17.   ./validation//images/0016.png\n",
      "18.   ./validation//images/0017.png\n",
      "19.   ./validation//images/0018.png\n",
      "20.   ./validation//images/0019.png\n",
      "21.   ./validation//images/0020.png\n",
      "22.   ./validation//images/0021.png\n",
      "23.   ./validation//images/0022.png\n",
      "24.   ./validation//images/0023.png\n",
      "25.   ./validation//images/0024.png\n",
      "26.   ./validation//images/0025.png\n",
      "27.   ./validation//images/0026.png\n",
      "28.   ./validation//images/0027.png\n",
      "29.   ./validation//images/0028.png\n",
      "30.   ./validation//images/0029.png\n",
      "31.   ./validation//images/0030.png\n",
      "32.   ./validation//images/0031.png\n",
      "33.   ./validation//images/0032.png\n",
      "34.   ./validation//images/0033.png\n",
      "35.   ./validation//images/0034.png\n",
      "36.   ./validation//images/0035.png\n",
      "37.   ./validation//images/0036.png\n",
      "38.   ./validation//images/0037.png\n",
      "39.   ./validation//images/0038.png\n",
      "40.   ./validation//images/0039.png\n",
      "41.   ./validation//images/0040.png\n",
      "42.   ./validation//images/0041.png\n",
      "43.   ./validation//images/0042.png\n",
      "44.   ./validation//images/0043.png\n",
      "45.   ./validation//images/0044.png\n",
      "46.   ./validation//images/0045.png\n",
      "47.   ./validation//images/0046.png\n",
      "48.   ./validation//images/0047.png\n",
      "49.   ./validation//images/0048.png\n",
      "50.   ./validation//images/0049.png\n",
      "51.   ./validation//images/0050.png\n",
      "52.   ./validation//images/0051.png\n",
      "53.   ./validation//images/0052.png\n",
      "54.   ./validation//images/0053.png\n",
      "55.   ./validation//images/0054.png\n",
      "56.   ./validation//images/0055.png\n",
      "57.   ./validation//images/0056.png\n",
      "58.   ./validation//images/0057.png\n",
      "59.   ./validation//images/0058.png\n",
      "60.   ./validation//images/0059.png\n",
      "61.   ./validation//images/0060.png\n",
      "62.   ./validation//images/0061.png\n",
      "63.   ./validation//images/0062.png\n",
      "64.   ./validation//images/0063.png\n",
      "65.   ./validation//images/0064.png\n",
      "66.   ./validation//images/0065.png\n",
      "67.   ./validation//images/0066.png\n",
      "68.   ./validation//images/0067.png\n",
      "69.   ./validation//images/0068.png\n",
      "70.   ./validation//images/0069.png\n",
      "71.   ./validation//images/0070.png\n",
      "72.   ./validation//images/0071.png\n",
      "73.   ./validation//images/0072.png\n",
      "74.   ./validation//images/0073.png\n",
      "75.   ./validation//images/0074.png\n",
      "76.   ./validation//images/0075.png\n",
      "77.   ./validation//images/0076.png\n",
      "78.   ./validation//images/0077.png\n",
      "79.   ./validation//images/0078.png\n",
      "80.   ./validation//images/0079.png\n",
      "81.   ./validation//images/0080.png\n",
      "82.   ./validation//images/0081.png\n",
      "83.   ./validation//images/0082.png\n",
      "84.   ./validation//images/0083.png\n",
      "85.   ./validation//images/0084.png\n",
      "86.   ./validation//images/0085.png\n",
      "87.   ./validation//images/0086.png\n",
      "88.   ./validation//images/0087.png\n",
      "89.   ./validation//images/0088.png\n",
      "90.   ./validation//images/0089.png\n",
      "91.   ./validation//images/0090.png\n",
      "92.   ./validation//images/0091.png\n",
      "93.   ./validation//images/0092.png\n",
      "94.   ./validation//images/0093.png\n",
      "95.   ./validation//images/0094.png\n",
      "96.   ./validation//images/0095.png\n",
      "97.   ./validation//images/0096.png\n",
      "98.   ./validation//images/0097.png\n",
      "99.   ./validation//images/0098.png\n",
      "100.   ./validation//images/0099.png\n"
     ]
    }
   ],
   "source": [
    "# Painting circles on images from validation loader and placing them in directory \"./validation/draw/\". \n",
    "# Notice that if the painted circle is seen only partly, it means that it is not inside the \n",
    "# [0,1]x[0,1] \"box\", which is the domain considered.  This means that your model has siginificant error\n",
    "\n",
    "\n",
    "paint_loader_circles(model, validation_loader, './validation/draw/')\n",
    "#paint_loader_circles(model, test_loader, './test/draw/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
