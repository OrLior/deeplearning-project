{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing your model works before submisson\n",
    "\n",
    "In order to make sure that your model will work when tested, you should run this notebook. Take the following steps:\n",
    "1. Have your 'model.dat' file placed in directory './saves/'. \n",
    "2. Paste the code of your CircleNet class in the second to last cell of this notebook. ** Make sure your CircleNet class code matches the code used to generate model.dat!**\n",
    "3. Run this whole notebook\n",
    "4. Verify that circles are painted in files located at \"./test/draw\" directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "from PIL import ImageDraw\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset_dir):\n",
    "        \"\"\"\n",
    "        Initializing dataset by generating a dicitonary of labels, where an image file name is the key \n",
    "        and its labels are the contents of that entry in the dictionary. Images are not loaded. This way it\n",
    "        is possible to iterate over arbitrarily large datasets (limited by labels dicitonary fitting \n",
    "        in memory, which is not a problem in practice)\n",
    "        \n",
    "        Args:\n",
    "            dataset_dir : path to directory with images and labels. In this directory we expect to find\n",
    "                          a directory called \"images\" containing the input images, and a file called \n",
    "                          \"labels.txt\" containing desired labels (coefficients)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.labels_dict = self.gen_labels_dict()\n",
    "        self.images_keys = list(self.labels_dict)  # getting the keys of the dictionary as list\n",
    "        self.images_keys.sort()                    # sorting so as to have in alphabetical order \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_dict)\n",
    "\n",
    "    def __getitem__(self, index):    \n",
    "        \"\"\"\n",
    "        This funtion makes it possible to iterate over the ShapesDataset\n",
    "        Args:\n",
    "            index: running index of images\n",
    "            \n",
    "        Returns:\n",
    "            sample: a dicitionary with three entries:\n",
    "                    1. 'image'  contains the image\n",
    "                    2. 'labels' contains labels (coeffs) corresponding to image\n",
    "                    3. 'fname'  contains name of file (image_key) - may be useful for debugging\n",
    "        \"\"\"\n",
    "        image_key = self.images_keys[index]     # recall - key is the file name of the corresponding image\n",
    "        image = np.array(Image.open(image_key)) # image has shape: (128, 128, 3)\n",
    "        image = image/255.0                     # simple normalization - just to maintain small numbers\n",
    "        image = np.transpose(image, (2, 0, 1))  # network needs RGB channels to be first index\n",
    "        labels = self.labels_dict[image_key]\n",
    "        sample = {'image': image, 'labels': labels, 'fname':image_key}\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    \n",
    "    def gen_labels_dict(self):\n",
    "        \"\"\"\n",
    "        This fucntion generates a dictionary of labels\n",
    "        \n",
    "        Returns:\n",
    "            labels_dict: the key is image file name and an array of labels is the corresponding contents \n",
    "        \"\"\"\n",
    "        \n",
    "        labels_fname = self.dataset_dir + \"/labels.txt\"\n",
    "        labels_dict = {}\n",
    "        with open(labels_fname, \"r\") as inp:\n",
    "            for line in inp:\n",
    "                line = line.split('\\n')[0]                                      # remove '\\n' from end of line \n",
    "                line = line.split(',')\n",
    "                key  = self.dataset_dir + '/images/' + line[0].strip() + \".png\" # image file name is the key\n",
    "                del line[0]\n",
    "                \n",
    "                list_from_line = [float(item) for item in line]\n",
    "                labels_dict[key] = np.asarray(list_from_line, dtype=np.float32)\n",
    "                        \n",
    "        return labels_dict             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = \"./test/\"  \n",
    "\n",
    "\n",
    "test_dataset = ShapesDataset(test_dir)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                          batch_size=1,\n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paint_loader_circles(model, loader, out_dir):\n",
    "    \"\"\"\n",
    "    This fucntion receives a model, a loader and an output directory. For each image in the loader it paints\n",
    "    a circle that the model identifies. The images are saved in the given out_dir diretory. \n",
    "    Args:\n",
    "        model   - network for idneitfying circles\n",
    "        loader  - input data to use \n",
    "        out_dir - ouptut directory name (e.g.: 'draws/'). If directory does not exist, it is created. If it exists,\n",
    "                  its files are deleted.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "                  # (dropout is set to zero)\n",
    "\n",
    "    k = 0\n",
    "    \n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "    files = glob.glob(out_dir + '*')\n",
    "    for f in files:\n",
    "        os.remove(f) \n",
    "  \n",
    "        \n",
    "    for data in loader:\n",
    "        # get inputs\n",
    "        inputs = (data['image']).to(device)\n",
    "        labels = (data['labels']).to(device)  # not using \n",
    "        img_fnames = data['fname'] \n",
    "      \n",
    "        \n",
    "        # forward\n",
    "        outputs = model(inputs.float())\n",
    "        curr_batch_size = np.shape(outputs)[0]\n",
    "        image_size = np.shape(inputs[0])  # image_size = [3, w, h]\n",
    "        _, width, height = image_size\n",
    "        assert (width == height)\n",
    "        \n",
    "        for i in range (curr_batch_size): \n",
    "            x0 = (outputs[i, 0].item()) * width\n",
    "            y0 = (1-outputs[i, 1].item()) * height\n",
    "            r  = outputs[i, 2].item() * width #assuming width=height here. Otherwise circle becomes ellipse.\n",
    "   \n",
    "            fname = img_fnames[i]\n",
    "            k+=1\n",
    "            print (str(k) + \".   \" + fname)\n",
    "\n",
    "            img = Image.open(fname)\n",
    "            draw = ImageDraw.Draw(img, 'RGBA')\n",
    "    \n",
    "            draw.ellipse((x0 - r, y0 - r, x0 + r ,y0 + r), fill=(160, 64, 0, 90), outline=None)\n",
    "    \n",
    "            img.save(out_dir + fname.split('/')[-1])\n",
    "    \n",
    "        \n",
    "    model.train()  #back to default\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your model definition is needed here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F  # a lower level (compared to torch.nn) interface\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class CircleNet(nn.Module):    # nn.Module is parent class  \n",
    "    def __init__(self):\n",
    "        super(CircleNet, self).__init__()  #calls init of parent class\n",
    "                \n",
    "\n",
    "        #----------------------------------------------\n",
    "        # implementation needed here \n",
    "        #----------------------------------------------\n",
    "      \n",
    "        \n",
    "        # The convolution layers were chosen to keep dimensions of input image: (I-F+2P)/S +1= (128-3+2)/1 + 1 = 128\n",
    "        \n",
    "        # Our images are RGB, so input channels = 3. Use 12 filters for first 2 convolution layers, then double\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=24, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        \n",
    "        \n",
    "        #Pooling to reduce sizes, and dropout to prevent overfitting\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.drop = nn.Dropout2d(p=0.2)\n",
    "#         self.norm1 = nn.BatchNorm2d(12)\n",
    "#         self.norm2 = nn.BatchNorm2d(24)\n",
    "        \n",
    "        # There are 2 pooling layers, each with kernel size of 2. Output size: 128/(2*2) = 32\n",
    "        # Feature tensors are now 32 x 32; With 24 output channels, this gives 32x32x24\n",
    "        \n",
    "        # Have 3 output features, corresponding to x-pos, y-pos, radius. \n",
    "        self.fc = nn.Linear(in_features=32 * 32 * 32, out_features=3)\n",
    "    \n",
    "\n",
    "                \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Feed forward through network\n",
    "        Args:\n",
    "            x - input to the network\n",
    "            \n",
    "        Returns \"out\", which is the network's output\n",
    "        \"\"\"\n",
    "        \n",
    "        #----------------------------------------------\n",
    "        # implementation needed here \n",
    "        #----------------------------------------------\n",
    "        \n",
    "        #Convolution 1\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.pool(out)\n",
    "        \n",
    "#         out = self.norm1(out)\n",
    "\n",
    "        #Convolution 2\n",
    "        out = self.conv2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.pool(out)\n",
    "        \n",
    "#         out = self.norm1(out)\n",
    "        \n",
    "        #Convolution 3\n",
    "        out = self.conv3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.drop(out)\n",
    "#         out = self.norm2(out)\n",
    "        \n",
    "        #Convolution 4\n",
    "        out = self.conv4(out)\n",
    "        out = self.relu(out)\n",
    "        out = F.dropout(out, training=self.training)\n",
    "        \n",
    "        \n",
    "        out = out.view(-1, 32 * 32 * 32)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        \n",
    "        return out\n",
    "             \n",
    "             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.   ./test//images/0000.png\n",
      "2.   ./test//images/0001.png\n",
      "3.   ./test//images/0002.png\n",
      "4.   ./test//images/0003.png\n",
      "5.   ./test//images/0004.png\n",
      "6.   ./test//images/0005.png\n",
      "7.   ./test//images/0006.png\n",
      "8.   ./test//images/0007.png\n",
      "9.   ./test//images/0008.png\n",
      "10.   ./test//images/0009.png\n",
      "11.   ./test//images/0010.png\n",
      "12.   ./test//images/0011.png\n",
      "13.   ./test//images/0012.png\n",
      "14.   ./test//images/0013.png\n",
      "15.   ./test//images/0014.png\n",
      "16.   ./test//images/0015.png\n",
      "17.   ./test//images/0016.png\n",
      "18.   ./test//images/0017.png\n",
      "19.   ./test//images/0018.png\n",
      "20.   ./test//images/0019.png\n",
      "21.   ./test//images/0020.png\n",
      "22.   ./test//images/0021.png\n",
      "23.   ./test//images/0022.png\n",
      "24.   ./test//images/0023.png\n",
      "25.   ./test//images/0024.png\n",
      "26.   ./test//images/0025.png\n",
      "27.   ./test//images/0026.png\n",
      "28.   ./test//images/0027.png\n",
      "29.   ./test//images/0028.png\n",
      "30.   ./test//images/0029.png\n",
      "31.   ./test//images/0030.png\n",
      "32.   ./test//images/0031.png\n",
      "33.   ./test//images/0032.png\n",
      "34.   ./test//images/0033.png\n",
      "35.   ./test//images/0034.png\n",
      "36.   ./test//images/0035.png\n",
      "37.   ./test//images/0036.png\n",
      "38.   ./test//images/0037.png\n",
      "39.   ./test//images/0038.png\n",
      "40.   ./test//images/0039.png\n",
      "41.   ./test//images/0040.png\n",
      "42.   ./test//images/0041.png\n",
      "43.   ./test//images/0042.png\n",
      "44.   ./test//images/0043.png\n",
      "45.   ./test//images/0044.png\n",
      "46.   ./test//images/0045.png\n",
      "47.   ./test//images/0046.png\n",
      "48.   ./test//images/0047.png\n",
      "49.   ./test//images/0048.png\n",
      "50.   ./test//images/0049.png\n",
      "51.   ./test//images/0050.png\n",
      "52.   ./test//images/0051.png\n",
      "53.   ./test//images/0052.png\n",
      "54.   ./test//images/0053.png\n",
      "55.   ./test//images/0054.png\n",
      "56.   ./test//images/0055.png\n",
      "57.   ./test//images/0056.png\n",
      "58.   ./test//images/0057.png\n",
      "59.   ./test//images/0058.png\n",
      "60.   ./test//images/0059.png\n",
      "61.   ./test//images/0060.png\n",
      "62.   ./test//images/0061.png\n",
      "63.   ./test//images/0062.png\n",
      "64.   ./test//images/0063.png\n",
      "65.   ./test//images/0064.png\n",
      "66.   ./test//images/0065.png\n",
      "67.   ./test//images/0066.png\n",
      "68.   ./test//images/0067.png\n",
      "69.   ./test//images/0068.png\n",
      "70.   ./test//images/0069.png\n",
      "71.   ./test//images/0070.png\n",
      "72.   ./test//images/0071.png\n",
      "73.   ./test//images/0072.png\n",
      "74.   ./test//images/0073.png\n",
      "75.   ./test//images/0074.png\n",
      "76.   ./test//images/0075.png\n",
      "77.   ./test//images/0076.png\n",
      "78.   ./test//images/0077.png\n",
      "79.   ./test//images/0078.png\n",
      "80.   ./test//images/0079.png\n",
      "81.   ./test//images/0080.png\n",
      "82.   ./test//images/0081.png\n",
      "83.   ./test//images/0082.png\n",
      "84.   ./test//images/0083.png\n",
      "85.   ./test//images/0084.png\n",
      "86.   ./test//images/0085.png\n",
      "87.   ./test//images/0086.png\n",
      "88.   ./test//images/0087.png\n",
      "89.   ./test//images/0088.png\n",
      "90.   ./test//images/0089.png\n",
      "91.   ./test//images/0090.png\n",
      "92.   ./test//images/0091.png\n",
      "93.   ./test//images/0092.png\n",
      "94.   ./test//images/0093.png\n",
      "95.   ./test//images/0094.png\n",
      "96.   ./test//images/0095.png\n",
      "97.   ./test//images/0096.png\n",
      "98.   ./test//images/0097.png\n",
      "99.   ./test//images/0098.png\n",
      "100.   ./test//images/0099.png\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"./saves/model.dat\")  # make sure you place here the path to your model\n",
    "\n",
    "paint_loader_circles(model, test_loader, './test/draw/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
